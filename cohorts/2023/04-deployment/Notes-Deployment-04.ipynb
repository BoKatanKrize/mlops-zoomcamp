{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ce88e50",
   "metadata": {},
   "source": [
    "## 4. Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53294cde",
   "metadata": {},
   "source": [
    "## 4.1 Three ways of deploying a model\n",
    "\n",
    "Recap of what we have done so far: \n",
    "1. We have designed the ML model\n",
    "2. Trained the model\n",
    "  - Experiment tracking\n",
    "  - Productionizing the model (ML pipeline)\n",
    "  \n",
    "And in this chapter, we select the model from the last step and **deploy** it. There are different ways of deploying a model depending on how frequently the prediction result is required:\n",
    "\n",
    "1. **(Offline) Batch deployment**: \n",
    "  - If we require the prediction less often and at regular intervals (e.g., we can wait for 1h, 1 day, 1 week for the prediction). \n",
    "  - The model is not up and running all the time but run somewhat regularly (hourly, daily, monthly)\n",
    "  - Overview:\n",
    "    - We have a database (with all our data)\n",
    "    - A scoring job (which stores the model) pulls some data and provides it to the model\n",
    "    - This data can be from the past hour, from yesterday, etc, depending on the regularity.\n",
    "    - Some predictions are made\n",
    "    - These are stored in another database\n",
    "  - An example with our Taxi project: \n",
    "    - The user has an app to call the taxi\n",
    "    - There are also other app competitors (e.g. uber)\n",
    "    - Our marketing team may call the model to study churn (how many users leaves us for the competitors)\n",
    "    - Marketing doesn't need to run the model all the time, but maybe just daily or weekly\n",
    "    \n",
    "2. **Online**\n",
    "  - For cases when we need predictions inmediately.\n",
    "  - The model is up and running all the time. It's always available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c8a7b2",
   "metadata": {},
   "source": [
    "Within the **online** mode, there are two options:\n",
    "\n",
    "1. **Web service**. \n",
    "  - You have a web service that contains the model. \n",
    "  - Through ```http``` requests we can get the prediction from the model\n",
    "  - 1x1 relationship (Client (the backend) -- Server (the web service)). The connection is kept alive while the server is processing the request and sends the response back\n",
    "  - Our project of predicting the duration of a taxi ride is a good case to use the web service:\n",
    "    - The user has an app which talks to the backend of the web service\n",
    "    - Info about the user is sent to the backend (time of the day, where is the user,...)\n",
    "    - This information is then used to run the model\n",
    "    - The predictions are sent back to the backend, which passes it to the user\n",
    "    - The user needs the predictions inmediately to make the decision of getting a taxi\n",
    "2. **Streaming**. \n",
    "  - In a streaming setting we have producers and consumers\n",
    "    - Producers: they generate events and push them to an event stream\n",
    "    - Consumers: they read from the stream and react to these events\n",
    "  - The key difference wrt to web service is the lack of explicit connection between Producers and Consumers. Producers don't care which or how many consumers there are\n",
    "  - 1xN or many-to-many relationship (one/many producers to many consumers)\n",
    "  - Applied to our taxi project:\n",
    "    - We can have one producer (backend) that the user interacts with.\n",
    "    - The producer then sends the event containing user info to a stream\n",
    "    - Multiple consumers (not to be confused with user) feed from this stream\n",
    "      - Consumer 1 could be user tip prediction (model to predict user tips). It would then send a notification to the user regarding the tip\n",
    "      - Consumer 2 may have a more accurate model to predict ride duration\n",
    "      - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b968a5",
   "metadata": {},
   "source": [
    "## 4.2 - Web-services: Deploying models with Flask and Docker\n",
    "\n",
    "- In this section we will use the pickle object saved in Week 1 and deploy it as a web application. \n",
    "- In the next section we will see how to connect our model registry from MLflow to the web application.\n",
    "\n",
    "We'll start first with an [introduction to Flask](https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/05-deployment/03-flask-intro.md) from the ml-zoomcamp course. Some descriptions first:\n",
    "\n",
    "- A **web service** allows different applications to communicate and exchange data over the internet using standardized protocols such as HTTP. It acts as a mediator, facilitating seamless data sharing regardless of the programming language they are written in.\n",
    "- **Flask** is a Python web framework. This means flask provides you with tools, libraries and technologies that allow you to build a web application.\n",
    "\n",
    "We can convert a Python function into a web service by using:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40175202",
   "metadata": {},
   "source": [
    "```python\n",
    "from flask import Flask\n",
    "\n",
    "\n",
    "# give an identity to your web service\n",
    "app = Flask('ping-pong')\n",
    "\n",
    "# Define a route ('/ping') for the web service. The decorated function \n",
    "# will be executed when that route is accessed.\n",
    "@app.route('/ping',methods=['GET'])\n",
    "def ping():\n",
    "    return 'PONG'\n",
    "\n",
    "# Run the Flask application (in an IDE) and start the web service\n",
    "if __name__=='__main__':\n",
    "    app.run(debug=True, host='0.0.0.0', port=9696)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52faf05",
   "metadata": {},
   "source": [
    "The ```ping()``` function is now converted into a web service using Flask. When you access the specified route (http://localhost:9696/ping), it will return the string 'PONG' as the response.\n",
    "\n",
    "In Flask, the ```@app.route``` decorator allows you to specify different HTTP methods for a particular route:\n",
    "- GET is a method used to retrieve data from the server. It's the default.\n",
    "- POST is used to send data to the server to create or update a resource (e.g. when login we are submitting (posting) our username and password to the web service). Note that there is no specification where the data goes.\n",
    "- PUT is same as POST but we are specifying where the data is going to.\n",
    "- DELETE is used to request to delete some data from the server.\n",
    "\n",
    "The ```app.run()``` method is used to customize the behavior of the Flask development server:\n",
    "- By default, it will run on the local machine (```127.0.0.1``` or ```localhost```)\n",
    "- It will run on port ```5000``` by default\n",
    "- The debug mode is enabled by default and provides helpful debugging information. It automatically reloads the server when code changes are detected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3667a12",
   "metadata": {},
   "source": [
    "After a brief introduction to Flask, we can now describe the steps to deploy a model as a web-service:\n",
    "\n",
    "1. Save the trained model \n",
    "2. Create a virtual environment\n",
    "3. Creating a script for predicting \n",
    "4. Putting the script into a Flask app\n",
    "5. Packaging the app to Docker\n",
    "\n",
    "The different files are located in ```notes-deployment-04-files/web-service```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da51148f",
   "metadata": {},
   "source": [
    "### 4.2.1 Save trained model\n",
    "\n",
    "We had saved the trained model using ```pickle``` (see Week 1). This resulted in the binary file ```lin_reg.bin```, which has been copied to ```notes-deployment-04-files/web-service```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339a226",
   "metadata": {},
   "source": [
    "### 4.2.2 Create a virtual environment\n",
    "\n",
    "We want to use the model developed in week 1 of the course. For that, we'll need to obtain the python environment we used to train and test the model for consistency. To obtain the packages and the package versions of the current python environment (shell command): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41910cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiosqlite==0.19.0\r\n",
      "alembic==1.11.1\r\n",
      "anyio==3.6.2\r\n",
      "appdirs==1.4.4\r\n",
      "apprise==1.4.0\r\n",
      "argon2-cffi==21.3.0\r\n",
      "argon2-cffi-bindings==21.2.0\r\n",
      "arrow==1.2.3\r\n",
      "asgi-lifespan==2.1.0\r\n",
      "asttokens==2.2.1\r\n",
      "asyncpg==0.27.0\r\n",
      "attrs==23.1.0\r\n",
      "backcall==0.2.0\r\n",
      "beautifulsoup4==4.12.2\r\n",
      "black==23.3.0\r\n",
      "bleach==6.0.0\r\n",
      "blinker==1.6.2\r\n",
      "boto3==1.26.139\r\n",
      "botocore==1.29.139\r\n",
      "cachetools==5.3.1\r\n",
      "certifi==2023.5.7\r\n",
      "cffi==1.15.1\r\n",
      "charset-normalizer==3.1.0\r\n",
      "click==8.1.3\r\n",
      "cloudpickle==2.2.1\r\n",
      "cmaes==0.9.1\r\n",
      "colorama==0.4.6\r\n",
      "colorlog==6.7.0\r\n",
      "comm==0.1.3\r\n",
      "contourpy==1.0.7\r\n",
      "coolname==2.2.0\r\n",
      "cramjam==2.6.2\r\n",
      "croniter==1.3.15\r\n",
      "cryptography==41.0.1\r\n",
      "cycler==0.11.0\r\n",
      "databricks-cli==0.17.7\r\n",
      "dateparser==1.1.8\r\n",
      "debugpy==1.6.7\r\n",
      "decorator==5.1.1\r\n",
      "defusedxml==0.7.1\r\n",
      "docker==6.1.2\r\n",
      "docker-pycreds==0.4.0\r\n",
      "entrypoints==0.4\r\n",
      "executing==1.2.0\r\n",
      "fastapi==0.96.0\r\n",
      "fastjsonschema==2.16.3\r\n",
      "fastparquet==2023.4.0\r\n",
      "Flask==2.3.2\r\n",
      "fonttools==4.39.4\r\n",
      "fqdn==1.5.1\r\n",
      "fsspec==2023.5.0\r\n",
      "future==0.18.3\r\n",
      "gitdb==4.0.10\r\n",
      "GitPython==3.1.31\r\n",
      "google-auth==2.19.1\r\n",
      "greenlet==2.0.2\r\n",
      "griffe==0.29.0\r\n",
      "gunicorn==20.1.0\r\n",
      "h11==0.14.0\r\n",
      "h2==4.1.0\r\n",
      "hpack==4.0.0\r\n",
      "httpcore==0.17.2\r\n",
      "httpx==0.24.1\r\n",
      "hyperframe==6.0.1\r\n",
      "hyperopt==0.2.7\r\n",
      "idna==3.4\r\n",
      "importlib-metadata==6.6.0\r\n",
      "ipykernel==6.23.1\r\n",
      "ipython==8.13.2\r\n",
      "ipython-genutils==0.2.0\r\n",
      "ipywidgets==8.0.6\r\n",
      "isoduration==20.11.0\r\n",
      "itsdangerous==2.1.2\r\n",
      "jedi==0.18.2\r\n",
      "Jinja2==3.1.2\r\n",
      "jmespath==1.0.1\r\n",
      "joblib==1.2.0\r\n",
      "jsonpatch==1.32\r\n",
      "jsonpointer==2.3\r\n",
      "jsonschema==4.17.3\r\n",
      "jupyter==1.0.0\r\n",
      "jupyter-console==6.6.3\r\n",
      "jupyter-events==0.6.3\r\n",
      "jupyter_client==8.2.0\r\n",
      "jupyter_core==5.3.0\r\n",
      "jupyter_server==2.5.0\r\n",
      "jupyter_server_terminals==0.4.4\r\n",
      "jupyterlab-pygments==0.2.2\r\n",
      "jupyterlab-widgets==3.0.7\r\n",
      "kiwisolver==1.4.4\r\n",
      "kubernetes==26.1.0\r\n",
      "Mako==1.2.4\r\n",
      "Markdown==3.4.3\r\n",
      "markdown-it-py==2.2.0\r\n",
      "MarkupSafe==2.1.2\r\n",
      "matplotlib==3.7.1\r\n",
      "matplotlib-inline==0.1.6\r\n",
      "mdurl==0.1.2\r\n",
      "mistune==2.0.5\r\n",
      "mlflow==2.3.2\r\n",
      "mypy-boto3-s3==1.26.127\r\n",
      "mypy-boto3-secretsmanager==1.26.135\r\n",
      "mypy-extensions==1.0.0\r\n",
      "nbclassic==1.0.0\r\n",
      "nbclient==0.7.4\r\n",
      "nbconvert==7.4.0\r\n",
      "nbformat==5.8.0\r\n",
      "nest-asyncio==1.5.6\r\n",
      "networkx==3.1\r\n",
      "notebook==6.5.4\r\n",
      "notebook_shim==0.2.3\r\n",
      "numpy==1.24.3\r\n",
      "oauthlib==3.2.2\r\n",
      "optuna==3.2.0\r\n",
      "orjson==3.9.0\r\n",
      "packaging==23.1\r\n",
      "pandas==2.0.1\r\n",
      "pandocfilters==1.5.0\r\n",
      "parso==0.8.3\r\n",
      "pathspec==0.11.1\r\n",
      "pathtools==0.1.2\r\n",
      "pendulum==2.1.2\r\n",
      "pexpect==4.8.0\r\n",
      "pickleshare==0.7.5\r\n",
      "Pillow==9.5.0\r\n",
      "platformdirs==3.5.1\r\n",
      "polars==0.17.14\r\n",
      "prefect==2.10.12\r\n",
      "prefect-aws==0.3.2\r\n",
      "prefect-email==0.2.2\r\n",
      "prometheus-client==0.16.0\r\n",
      "prompt-toolkit==3.0.38\r\n",
      "protobuf==4.23.1\r\n",
      "psutil==5.9.5\r\n",
      "ptyprocess==0.7.0\r\n",
      "pure-eval==0.2.2\r\n",
      "py4j==0.10.9.7\r\n",
      "pyarrow==11.0.0\r\n",
      "pyasn1==0.5.0\r\n",
      "pyasn1-modules==0.3.0\r\n",
      "pycparser==2.21\r\n",
      "pydantic==1.10.9\r\n",
      "Pygments==2.15.1\r\n",
      "PyJWT==2.7.0\r\n",
      "pyparsing==3.0.9\r\n",
      "pyrsistent==0.19.3\r\n",
      "python-dateutil==2.8.2\r\n",
      "python-json-logger==2.0.7\r\n",
      "python-slugify==8.0.1\r\n",
      "pytz==2023.3\r\n",
      "pytzdata==2020.1\r\n",
      "PyYAML==6.0\r\n",
      "pyzmq==25.0.2\r\n",
      "qtconsole==5.4.3\r\n",
      "QtPy==2.3.1\r\n",
      "querystring-parser==1.2.4\r\n",
      "readchar==4.0.5\r\n",
      "regex==2023.6.3\r\n",
      "requests==2.31.0\r\n",
      "requests-oauthlib==1.3.1\r\n",
      "rfc3339-validator==0.1.4\r\n",
      "rfc3986-validator==0.1.1\r\n",
      "rich==13.4.1\r\n",
      "rsa==4.9\r\n",
      "s3transfer==0.6.1\r\n",
      "scikit-learn==1.2.2\r\n",
      "scipy==1.10.1\r\n",
      "seaborn==0.12.2\r\n",
      "Send2Trash==1.8.2\r\n",
      "sentry-sdk==1.25.0\r\n",
      "setproctitle==1.3.2\r\n",
      "six==1.16.0\r\n",
      "smmap==5.0.0\r\n",
      "sniffio==1.3.0\r\n",
      "soupsieve==2.4.1\r\n",
      "SQLAlchemy==2.0.15\r\n",
      "sqlparse==0.4.4\r\n",
      "stack-data==0.6.2\r\n",
      "starlette==0.27.0\r\n",
      "tabulate==0.9.0\r\n",
      "terminado==0.17.1\r\n",
      "text-unidecode==1.3\r\n",
      "threadpoolctl==3.1.0\r\n",
      "tinycss2==1.2.1\r\n",
      "toml==0.10.2\r\n",
      "tomli==2.0.1\r\n",
      "tornado==6.3.2\r\n",
      "tqdm==4.65.0\r\n",
      "traitlets==5.9.0\r\n",
      "typer==0.9.0\r\n",
      "typing_extensions==4.6.1\r\n",
      "tzdata==2023.3\r\n",
      "tzlocal==5.0.1\r\n",
      "uri-template==1.2.0\r\n",
      "urllib3==1.26.16\r\n",
      "uvicorn==0.22.0\r\n",
      "wandb==0.15.3\r\n",
      "wcwidth==0.2.6\r\n",
      "webcolors==1.13\r\n",
      "webencodings==0.5.1\r\n",
      "websocket-client==1.5.1\r\n",
      "websockets==11.0.3\r\n",
      "Werkzeug==2.3.4\r\n",
      "widgetsnbextension==4.0.7\r\n",
      "xgboost==1.7.5\r\n",
      "zipp==3.15.0\r\n"
     ]
    }
   ],
   "source": [
    "! pip freeze "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b0e705",
   "metadata": {},
   "source": [
    "We're mostly interested in getting the scikit-learn version:\n",
    "\n",
    "```scikit-learn==1.2.2```\n",
    "\n",
    "Now that we have the package version we need. We use ```pipenv``` to create an envionment with them in ```notes-deployment-04-files/web-service``` (on the 'base' ```conda``` environment):\n",
    "\n",
    "```\n",
    "$ pip install pipenv\n",
    "$ pipenv install scikit-learn==1.2.2 flask --python=3.10.9\n",
    "$ pipenv shell\n",
    "$ exit\n",
    "```\n",
    "\n",
    "Notes: \n",
    "- ```pip install``` only in case ```pipenv``` is not installed\n",
    "- ```pipenv``` uses the current directory as the root of the environment\n",
    "- ```pipenv shell``` activates the environment\n",
    "- ```exit``` exits the environment\n",
    "\n",
    "This creates two files in our directory: ```Pipfile``` and ```Pipfile.lock```. \n",
    "- ```Pipfile``` stores the versions of the packages that we want (like scikit-learn, Flask) \n",
    "-```Pipfile.lock``` stores the dependency tree to avoid for example updating Numpy for scikit-learn and breaking Flask in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ccaef6",
   "metadata": {},
   "source": [
    "### 4.2.3 Create script for predicting\n",
    "\n",
    "We create a simple script that loads the saved model, preprocesses the input data and generates prediction (```predict.py```). The following paragraphs refer to this file.\n",
    "\n",
    "In Week 1 we pickled 2 files into ```lin_reg.bin```:\n",
    "1. ```DictVectorizer```\n",
    "2. Linear Regressor\n",
    "\n",
    "They will be loaded with:\n",
    "\n",
    "```python\n",
    "with open('lin_reg.bin', 'rb') as f_in:\n",
    "    (dv, model) = pickle.load(f_in)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb745fb9",
   "metadata": {},
   "source": [
    "We then extract and preprocess the features as we did in Week 1 by using the function\n",
    "\n",
    "```python\n",
    "def prepare_features(ride: dict[str,float]) -> dict[str,float|str]:\n",
    "    ...\n",
    "```\n",
    "\n",
    "And we perform the prediction (it will only return 1st prediction) \n",
    "```python\n",
    "def predict(features: dict[str,float|str]) -> float:\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0e0714",
   "metadata": {},
   "source": [
    "### 4.2.4 Put the script into a Flask app\n",
    "\n",
    "Now that we have the ```predict.py``` ready, we can convert it into a Flask app:\n",
    "\n",
    "```python\n",
    "app = Flask('duration-prediction')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict_endpoint(): # <-- The parameters are usually given by Flask\n",
    "  ride = request.get_json() # <-- The parameters are extracted from the\n",
    "                            #     request (reads the JSON passed to the app)\n",
    "  features = prepare_features(ride)\n",
    "  pred = predict(features)\n",
    "  \n",
    "  result = {'duration': pred}\n",
    "    \n",
    "  return jsonify(result)  # transforms a dictionary into a JSON\n",
    "```\n",
    "\n",
    "To run the Flask application on localhost we add to the previous file:\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "  app.run(debug=True, host='0.0.0.0', port=9696)\n",
    "```\n",
    "Now if we run ```predict.py```, a Flask application will run on ```localhost``` on port ```9696```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb23bc7",
   "metadata": {},
   "source": [
    "#### Request Predictions from the Flask app\n",
    "\n",
    "To request a prediction from the server, we create another file ```test.py```. This file will post its ride information to the server and print out the response (i.e: the predicted duration):\n",
    "\n",
    "```$ python test.py```\n",
    "\n",
    "Output:\n",
    "\n",
    "```{'duration': 25.82088225071811}```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f51637a",
   "metadata": {},
   "source": [
    "#### Use a production WSGI server\n",
    "\n",
    "The current Flask setup is a development environment. We receive the following warning:\n",
    "\n",
    "```WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.```\n",
    "\n",
    "To deploy the model into production, we use ```gunicorn```:\n",
    "\n",
    "```\n",
    "$ pipenv install gunicorn\n",
    "$ gunicorn --bind=0.0.0.0:9696 predict:app\n",
    "```\n",
    "\n",
    "where predict is the ```predict.py``` located in the current directory, and ```app``` is the Flask app defined on that file (see above). Now we run the same application, but instead of Flask we use ```gunicorn```. We can again run ```test.py``` and confirm that we get the same result.\n",
    "\n",
    "We have run ```test.py``` script from the ```mlops-zoomcamp``` conda environment (development environment). The ```requests``` library is installed there. Ideally, the development environment should have the ```requests``` library installed as we need to do the testing, however in the production environment we do not need to install it.\n",
    "\n",
    "To run ```test.py``` from the ```pipenv``` (production environment) we can install ```requests``` with dev dependencies only so that it will not be available during deployment:\n",
    "\n",
    "```\n",
    "$ pipenv install --dev requests\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb252ebe",
   "metadata": {},
   "source": [
    "### 4.2.5 Package the app to Docker\n",
    "\n",
    "Now we want to deploy our predictor into a Docker container. We create the following ```Dockerfile``` in ```notes-deployment-04-files/web-service```:\n",
    "\n",
    "\n",
    "```dockerfile\n",
    "# Use the base image of Python version 3.10.9 with a slim distribution\n",
    "# (smaller footprint compared to the regular distribution)\n",
    "FROM python:3.10.9-slim\n",
    "# Update pip to the latest version\n",
    "RUN pip install -U pip\n",
    "# Install pipenv package manager\n",
    "RUN pip install pipenv\n",
    "# Set the working directory inside the container to /app\n",
    "WORKDIR /app\n",
    "# Copy the Pipfile and Pipfile.lock from the local directory to the container's\n",
    "# /app directory\n",
    "COPY [ \"Pipfile\", \"Pipfile.lock\", \"./\" ]\n",
    "# Install the project dependencies. They are installed on the system python \n",
    "# (w/o virtual environment) as Docker already gives us the isolation. \n",
    "# Additionally, --deploy insures that pipenv will install the exact versions of the \n",
    "# dependencies as recorded in Pipfile.lock to ensure consistency and reproducibility\n",
    "# when installing dependencies across different environments \n",
    "RUN pipenv install --system --deploy\n",
    "# Copy the predict.py and lin_reg.bin files from the local directory to the \n",
    "# container's /app directory\n",
    "COPY [ \"predict.py\", \"lin_reg.bin\", \"./\" ]\n",
    "# Expose port 9696 for the container to listen on\n",
    "EXPOSE 9696\n",
    "# Set the entrypoint command to run gunicorn with the specified bind address and \n",
    "# port, and the \"predict:app\" application\n",
    "ENTRYPOINT [ \"gunicorn\", \"--bind=0.0.0.0:9696\", \"predict:app\" ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c4e026",
   "metadata": {},
   "source": [
    "We then build the Docker Image with:\n",
    "\n",
    "```\n",
    "$ docker build -t ride-duration-prediction-service:v1 .\n",
    "```\n",
    "where:\n",
    "- The ```-t``` flag is used to specify a tag for the Docker image.\n",
    "- The dot ```.``` at the end specifies that the current directory should be used as the build context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b74ead",
   "metadata": {},
   "source": [
    "And run the container with:\n",
    "\n",
    "```\n",
    "$ docker run -it --rm -p 9696:9696 ride-duration-prediction-service:v1\n",
    "```\n",
    "where:\n",
    "- ```-it``` enables interactive mode in the container, allowing you to interact with the container's terminal.\n",
    "- ```--rm``` indicates that the container should be automatically removed when it exits.\n",
    "- ```-p``` is used to publish and map ports between the container and the host. In this case, it maps port ```9696``` of the host to port ```9696``` of the container.\n",
    "\n",
    "This will deploy the app on ```localhost```. We can run the ```test.py``` script again to confirm the result. Now instead of going to ```guincorn``` or Flask, it goes to the model deployed in a Docker container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4548d0cd",
   "metadata": {},
   "source": [
    "## 4.3 Web-services: Getting the models from the model registry (MLflow)\n",
    "\n",
    "Up to this point, we have prepared the model in a ```Dockerfile```, making it deployable on any Docker-compatible computing platform. However, the model we utilized was retrieved directly from a local path, which contradicts what we learned in previous sessions. We were advised to utilize a model registry (MLflow) to store the candidate models. Therefore, in this section, we will explore how to retrieve the model from the model registry for serving purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d23a6d",
   "metadata": {},
   "source": [
    "### 4.3.1 Setup MLflow\n",
    "\n",
    "We launch the mlflow server locally by running the following command in your terminal while being in ```/notes-deployment-04-files/web-service-mlflow```:\n",
    "\n",
    "```\n",
    "(mlops-zoomcamp) $ mlflow server --backend-store-uri sqlite:///backend.db --default-artifact-root ./artifacts_local\n",
    "```\n",
    "\n",
    "The artifacts will be saved in ```./artifacts_local``` but the runs and metadata will be stored in the sqlite database (there will be a file ```backend.db``` specifying that sqlite has been used). In this scenario we can use the model registry.\n",
    "\n",
    "Now we can open mlflow UI on http://127.0.0.1:5000/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a30fcf",
   "metadata": {},
   "source": [
    "### 4.3.2 Train a model\n",
    "\n",
    "We train a Random Forest regressor model, and tracked and saved the model in MLflow (```random-forest.py```). You can check the experiment details and logged model artifact in MLflow UI:\n",
    "\n",
    "![title](images/mlflow1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247ddc7f",
   "metadata": {},
   "source": [
    "We can then extract the run ID: \n",
    "\n",
    "```python\n",
    "RUN_ID = '068bda10a3ed4b73a771df771161f60a'\n",
    "```\n",
    "and save it in ```web-service-mlflow/predict.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18a1439",
   "metadata": {},
   "source": [
    "### 4.3.3 Inference script to fetch model from MLflow\n",
    "\n",
    "We copy the ```Pipfile``` and ```Pipfile.lock``` from the previous section (4.2) to our current folder (```web-service-mlflow```). Then we use ```pipenv``` to create an environment in this folder. It will inherit the modules from the ```Pipfile``` and ```Pipfile.lock``` files already present. We also want to install MLflow:\n",
    "\n",
    "```\n",
    "$ pipenv install mlflow\n",
    "```\n",
    "\n",
    "Now if we run ```web-service-mlflow/predict.py```, a Flask application will run on ```localhost``` on port ```9696```. To request a prediction from the server, we run ```web-service-mlflow/test.py```. This file will post its ride information to the server and print out the response (i.e: the predicted duration + run_id):\n",
    "\n",
    "```$ python test.py```\n",
    "\n",
    "Output:\n",
    "\n",
    "```{'duration': 45.50965007660852, 'model_version': '068bda10a3ed4b73a771df771161f60a'}```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74bab3f",
   "metadata": {},
   "source": [
    "## 4.4 (Optional) Streaming: Deploying models with Kinesis and Lambda\n",
    "\n",
    "See these [Notes](https://sagarthacker.com/posts/mlops/aws-deployment-lambda-kinesis.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0ab2bb",
   "metadata": {},
   "source": [
    "## 4.5 Batch deployment: use of scoring script\n",
    "\n",
    "Even though **batch deployment** is not the ideal manner to deploy our \"taxi ride duration\" model (ideal way will be web service), we can rethink the problem as to having the *actual* duration and the *predicted* duration and see how often our drivers deviate from the *ideal* predicted duration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb7ea2a",
   "metadata": {},
   "source": [
    "### 4.5.1 Preparing a scoring script\n",
    "\n",
    "We start from the ```web-service-mlflow/random-forest.ipynb``` notebook from the previous dection 4.3:\n",
    "- We copy it and rename it to ```batch/score.ipynb```.\n",
    "- We add the first lines from ```web-service-mlflow/predict.py``` (related to MLflow and loading the model created in 4.3)\n",
    "- To check that the model is loaded correctly, we need to start MLflow in the directory ```web-service-mlflow/```, where we saved all artifacts.\n",
    "\n",
    "```\n",
    "(mlops-zoomcamp) $ mlflow server --backend-store-uri sqlite:///backend.db --default-artifact-root ./artifacts_local\n",
    "```\n",
    "- Now we'll just apply the model (we do not train), therefore we just have ```df```, ```dicts``` and the prediction on the loaded model ```y_pred```\n",
    "- We save the predictions in a DataFrame. To give an unique ID to every row we can use the Python library ```uuid```\n",
    "- We add some meta information to this DataFrame ('PULocationID','DOLocationID','actual_duration',...)\n",
    "- We can also parametrise the input/output files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddce06c3",
   "metadata": {},
   "source": [
    "### 4.5.2 Running the scoring script\n",
    "\n",
    "To convert the notebook into a Python script, execute:\n",
    "\n",
    "```\n",
    "$ jupyter nbconvert --to script score.ipynb\n",
    "```\n",
    "\n",
    "which will create ```score.py``` in the current directory (```batch/```). You can now create a function \n",
    "\n",
    "```python\n",
    "def run():\n",
    "...\n",
    "```\n",
    "to parametrice the script. We include the module ```sys``` to read the parameters from the terminal:\n",
    "- ```taxi_type``` is the 1st parameter\n",
    "- ```run_id``` is the 2nd parameter\n",
    "- ```output_file``` is the 3rd parameter\n",
    "- ```run_id``` is the 4th parameter\n",
    "\n",
    "Therefore, by running:\n",
    "```\n",
    "$ python score.py green 2021 2 068bda10a3ed4b73a771df771161f60a\n",
    "```\n",
    "we get the output file ```green_2021-02.parquet``` stored in ```batch/output/```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0dbab3",
   "metadata": {},
   "source": [
    "## 4.6 Batch deployment: Scheduling scoring jobs with Prefect\n",
    "\n",
    "### 4.6.1 Adjusting previous ```score.py``` file\n",
    "We will start by copying the previous file ```batch/score.py``` to the folder to a new folder ```batch-prefect/score.py```. Some changes have been pre-included in this new directory :\n",
    "- Added function ```save_results()``` -> deals with all operations in ```df_result```\n",
    "- Added Prefect ```@task``` to the function ```apply_model```\n",
    "- ```print()``` has been replaced by Prefect's ```get_run_logger()```\n",
    "- Added function ```get_paths()``` -> deals with input/output paths\n",
    "- Added function ```ride_duration_prediction()``` decorated with Prefect's ```@flow``` -> repackages some of the functionality of ```run()```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac09d9d",
   "metadata": {},
   "source": [
    "### 4.6.2 Run new ```score.py``` with Prefect support\n",
    "\n",
    "We are ready to run the new file!\n",
    "\n",
    "- To check that the model is loaded correctly, we need to start MLflow in the directory ```web-service-mlflow/```, where we saved all artifacts.\n",
    "\n",
    "```\n",
    "(mlops-zoomcamp) $ mlflow server --backend-store-uri sqlite:///backend.db --default-artifact-root ./artifacts_local\n",
    "```\n",
    "We can now run again:\n",
    "```\n",
    "$ python score.py green 2021 2 068bda10a3ed4b73a771df771161f60a\n",
    "```\n",
    "we get the output file ```green_2021-01.parquet``` stored in ```batch-prefect/output/```. We can also see the logs in Prefect:\n",
    "\n",
    "![title](images/prefect1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f241ad8a",
   "metadata": {},
   "source": [
    "### 4.6.3 Create Prefect Project & deployment\n",
    "\n",
    "In order to proceed with the deployment, we will use Prefect Projects to manage and maintain our different deployments. By using Projects we add another layer of abstraction (group different deployments together). To initialize a Project within ```batch-prefect/```:\n",
    "\n",
    "```(mlops-zoomcamp) $ prefect project init```\n",
    "\n",
    "which will create different ```.yaml``` files within the directory to set the project up and running. \n",
    "\n",
    "1. We can now create a work pool in the Prefect UI. We name it ```local-work```\n",
    "\n",
    "2. We get the desired flow (```score.py:ride_duration_prediction```) deployed by running:\n",
    "\n",
    "```\n",
    "(mlops-zoomcamp) $ prefect deploy score.py:ride_duration_prediction -n my-first-deployment -p local-work\n",
    "```\n",
    "\n",
    "We can see now the new deployment in Prefect UI:\n",
    "\n",
    "![title](images/prefect2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c9f6b",
   "metadata": {},
   "source": [
    "### 4.6.4 Running deployment\n",
    "\n",
    "1. To execute flow runs from the deployment set up in 4.6.3, start a worker that pulls work from \n",
    "the ```local-work``` work pool\n",
    "\n",
    "```\n",
    "(mlops-zoomcamp) $ prefect worker start --pool local-work\n",
    "```\n",
    "\n",
    "2. Now we can go to the deployment and launch a quick run. A window will pop up asking for the input parameters:\n",
    "\n",
    "![title](images/prefect3.png)\n",
    "\n",
    "**Important**: Make sure that the data you are using is uploaded to your GitHub repo, otherwise it will not find the files and raise an error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
