{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83bf9188",
   "metadata": {},
   "source": [
    "## 3.1 Introdution to Workflow Orchestration\n",
    "\n",
    "The image below displays an example of a MLOps Workflow:\n",
    "\n",
    "- Some data (PostgreSQL database) is fetched by Python code\n",
    "- Pandas is used to preprocess the data (cleaning, EDA, etc.)\n",
    "    - Data is then saved to a parquet file (to be read later, etc.)\n",
    "- Scikit-Learn is then used for feature engineering or running some models\n",
    "- Or we can also use XGBoost \n",
    "- At the same time, MLflow is tracking everything \n",
    "- Finally, we deploy the model using Flask "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4597c2",
   "metadata": {},
   "source": [
    "![title](images/workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8795d672",
   "metadata": {},
   "source": [
    "### Possible complications\n",
    "\n",
    "#### a) Failures along the workflow\n",
    "\n",
    "We may have failure points at lots of different parts in the workflow. The goal of the workflow orchestration is to minimize the errors and fail gracefully.\n",
    "\n",
    "\n",
    "#### b) Complex interdependency\n",
    "You may be asked to:\n",
    "- Could you just set up this pipeline to train this model? (add another extra layer)\n",
    "- Could you set up some logging? (e.g., outside MLflow)\n",
    "- Could you do it every day? (set up a schedule)\n",
    "- Could you make it retry if it fails?\n",
    "- Could you send me a message when it succeeds? (e.g., email)\n",
    "- Could you visualize the dependencies?\n",
    "- Could you add caching? (so we don't need to re-run everytime)\n",
    "- Could you add some collaborators to run ad-hoc, and who don't code?\n",
    "\n",
    "#### Solution\n",
    "This is where **Prefect** comes in. It allows you to orchestrate and observe your Python workflows at scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88da3107",
   "metadata": {},
   "source": [
    "## 3.2 Introduction to Prefect\n",
    "We are going to see:\n",
    "- how Python code can be converted to a Prefect script\n",
    "- run our own Prefect server locally \n",
    "- run a few Prefect flows against the server\n",
    "- Interact with Prefect UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3775331",
   "metadata": {},
   "source": [
    "### What is Prefect?\n",
    "Prefect is a flexible, open-source Python framework that can be leveraged to turn python code into robust workflows that are more resilient to downtime and unforeseen failures.\n",
    "\n",
    "When self-hosting a Prefect Server we can separate three different components:\n",
    "- **Orchestration API**: it's a REST API that is used by the server to work with workflow metadata (so that you can create, control, and monitor the execution of workflows efficiently)\n",
    "- this workflow metadata is stored in a **Database** (SQLite by default)\n",
    "- the workflows can be visualized thanks to the server **UI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48b5a23",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "The two basic building blocs of Prefect are:\n",
    "- **Task:** A discrete unit of work in a Prefect workflow. Tasks in Prefect can encapsulate any Python code or callable, such as functions, methods, or classes by using the decorator ```@task```\n",
    "- **Flow:** It provides a way to define, organize, and orchestrate the execution of tasks in a specific order. You can turn any function into a Prefect flow by adding the ```@flow``` decorator.\n",
    "- **Subflow:** Flow called by another flow. There is no command to make this explicit, but we can use the name argument in the decorator:\n",
    "```@flow(name='Subflow')```\n",
    "\n",
    "*Important!:* All tasks must be called from within a flow. Tasks may not be called from other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6895e0e0",
   "metadata": {},
   "source": [
    "### Installing Prefect\n",
    "In a conda environment, install all package dependencies with\n",
    "```\n",
    "$ pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Start the Prefect server locally\n",
    "\n",
    "Activate your conda environment with the new packages installed. Start the Prefect API server locally with\n",
    "```\n",
    "$ prefect server start\n",
    "```\n",
    "Once the Prefect server is up and running, we make sure that we apply the API URL to our Prefect configuration so that we're pointing to the correct API URL and the workflow metadata is correctly sent to the server UI. Therefore, in a new CLI window, we run the following command:\n",
    "```\n",
    "$ prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api\n",
    "```\n",
    "and then we can access Prefect UI at ```http://127.0.0.1:4200```. It will look like this:\n",
    "\n",
    "![title](images/prefect-UI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13561eb9",
   "metadata": {},
   "source": [
    "### Launch a flow run\n",
    "\n",
    "As an example, we can run separately the Python script ```cohorts/2023/03-orchestration/3.2/cat_facts.py```. This script fetches some random trivia about cats from the web and from time to time it will give ```500 Internal Server Error```. \n",
    "\n",
    "Now we have collected a new flow run in the UI. The new flow will be under the name of the function it's decorating (```fetch``` in this case):\n",
    "\n",
    "![title](images/Prefect-firstrun.png)\n",
    "\n",
    "We can see in the logs (also available in the CLI when we run the script) that an exception happenend and that the task has been retried sucessfully. Due to the random nature of the script, different logs will be recorded for each time the script is executed. \n",
    "\n",
    "![title](images/Prefect-firstrun-logs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231b984",
   "metadata": {},
   "source": [
    "### Example of subflows\n",
    "\n",
    "Now we can run the script ```cohorts/2023/03-orchestration/3.2/cat_dog_facts.py``` to see subflows in action. We have a parent flow (```animal-facts```) that calls two subflows (```fetch-cat-fact``` and ```fetch-dog-fact```). Therefore, we will have 3 flow records: \n",
    "\n",
    "![title](images/prefect-subflows.png)\n",
    "\n",
    "The log print argument is set in the parent:\n",
    "\n",
    "![title](images/prefect-subflows-logs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e48ca28",
   "metadata": {},
   "source": [
    "## 3.3 Prefect Workflow\n",
    "\n",
    "In this section, we will learn how to productionize the Python scripts from last week's ```02-experiment-tracking```:\n",
    "- ```preprocess_data.py```\n",
    "- ```train.py```\n",
    "- ```hpo.py```\n",
    "\n",
    "We will skip the hyperparameter tunning step and focus on the best performing model.  We put everything together in the script ```orchestrate.py``` and add some observability and orchestration. This script is organized as follows:\n",
    "\n",
    "The main flow function\n",
    "\n",
    "```python\n",
    "@flow\n",
    "def main_flow(\n",
    "    train_path: str = \"../data/green_tripdata_2021-01.parquet\",\n",
    "    val_path: str = \"../data/green_tripdata_2021-02.parquet\",\n",
    ") -> None:\n",
    "    \"\"\"The main training pipeline\"\"\"\n",
    "    \n",
    "    ...\n",
    "```\n",
    "\n",
    "It collects the following tasks:\n",
    "   1) sets up MLflow\n",
    "   2) **reads** the train and validation data\n",
    "   ```python\n",
    "@task(retries=3, retry_delay_seconds=2)\n",
    "def read_data(filename: str) -> pl.DataFrame:\n",
    "    \"\"\"Read data into DataFrame\"\"\"\n",
    "    ...\n",
    "```\n",
    "   3) **performs some transformations** on the data \n",
    "   ```python\n",
    "@task\n",
    "def add_features(\n",
    "    df_train: pl.DataFrame, df_val: pl.DataFrame\n",
    ") -> tuple[\n",
    "          scipy.sparse.spmatrix,\n",
    "          scipy.sparse.spmatrix,\n",
    "          np.ndarray,\n",
    "          np.ndarray,\n",
    "          sklearn.feature_extraction.DictVectorizer,\n",
    "    ]:\n",
    "    \"\"\"Add features to the model\"\"\"\n",
    "    ...\n",
    "```\n",
    "\n",
    "   4) and **trains** the best model\n",
    "   ```python\n",
    "@task(log_prints=True)\n",
    "def train_best_model(\n",
    "          X_train: scipy.sparse.spmatrix,\n",
    "          X_val: scipy.sparse.spmatrix,\n",
    "          y_train: np.ndarray,\n",
    "          y_val: np.ndarray,\n",
    "          dv: sklearn.feature_extraction.DictVectorizer,\n",
    ") -> None:\n",
    "    \"\"\"train a model with best hyperparams and write everything out\"\"\"\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676ef950",
   "metadata": {},
   "source": [
    "We can now run ```orchestrate.py``` and go to the Prefect UI:\n",
    "\n",
    "![title](images/prefect-orchestration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84454c3",
   "metadata": {},
   "source": [
    "## 3.4 Deploying your Workflow\n",
    "\n",
    "In this section we will present how to deploy workflows using Prefect Projects for productionizing. It will allows to do scheduling and collaborations with others (e.g. using Prefect Cloud).\n",
    "\n",
    "\n",
    "### Create a Prefect Project\n",
    "A project is a minimally opinionated set of files that describe how to prepare one or more flow deployments. It can be initialized by running the CLI command \n",
    "\n",
    "```$ prefect project init```\n",
    "\n",
    "in the **root directory** ```mlops-zoomcamp```. Now the following files has been created:\n",
    "- ```deployment.yaml```: a YAML file describing base settings for multiple deployments from this project.\n",
    "- ```prefect.yaml```: a YAML file that contains default instructions for how to build and push any necessary code artifacts (such as Docker images) from this project, as well as default instructions for pulling a deployment in remote execution environments (e.g., cloning a GitHub repository)\n",
    "- ```.prefect/```: a hidden directory where Prefect will store workflow metadata\n",
    "- ```.prefectignore```: to skip upload of some files which are relevant for local development but not intended for use in production.\n",
    "\n",
    "*Important 1*: when you run ```prefect project init```, the files above are not overwritten, so you need to remove them manually.\n",
    "\n",
    "*Important 2*: I made sure that the project is initiated in the root directory because when running the deployment it will clone ```mlops-zoomcamp.git``` from a remote location (e.g. GitHub). See ```prefect.yaml```. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90b9902",
   "metadata": {},
   "source": [
    "### Create a Work Pool\n",
    "\n",
    "Work pools, together with workers and agents, bridge the Prefect orchestration environment with your execution environment. \n",
    "\n",
    "When a deployment creates a flow run, it is submitted to a specific work pool for scheduling. A worker or agent running in the execution environment polls its respective work pool for new runs to execute.\n",
    "\n",
    "We can create work pools by going to the UI:\n",
    "\n",
    "![title](images/work-pools.png)\n",
    "\n",
    "1. Basic information : e.g. Name: zoompool\n",
    "2. Infrastructure Type: Local Subprocess\n",
    "3. Configuration: leave as it is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02893847",
   "metadata": {},
   "source": [
    "### Deploy your flow\n",
    "A deployment is a server-side concept that encapsulates a flow, allowing it to be scheduled and triggered via API. The deployment stores metadata about where your flow's code is stored and how your flow should be run.\n",
    "\n",
    "At a high level, you can think of a deployment as configuration for managing flows, whether you run them via the CLI, the UI, or the API.\n",
    "\n",
    "To deploy ```orchestrate.py```, run:\n",
    "```\n",
    "$ prefect deploy cohorts/2023/03-orchestration/3.4/orchestrate.py:main_flow -n taxi1 -p zoompool\n",
    "```\n",
    "while being at the root directory ```mlops-zoomcamp``` folder. \n",
    "\n",
    "- ```main_flow``` refers to the flow within the Python script that we want as the entrypoint function for the deployment.\n",
    "- ```-n``` gives a name to the deployment, in this case *taxi1*\n",
    "- ```-p``` assigns a work pool to the deployment, in this case *zoompool*\n",
    "\n",
    "Now we can find the deployment in the UI:\n",
    "\n",
    "![title](images/prefect-deployment.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e822560b",
   "metadata": {},
   "source": [
    "### Start a worker\n",
    "\n",
    "To run deployments, you must start a worker or agent that pulls work from the *zoompool* work pool.\n",
    "\n",
    "- **agent**: Agent processes are lightweight polling services that get scheduled work from a work pool and deploy the corresponding flow runs\n",
    "- **worker** : Workers are similar to agents, but offer greater control over infrastructure configuration and the ability to route work to specific types of execution environments.\n",
    "\n",
    "To start a worker:\n",
    "```\n",
    "$ prefect worker start -p zoompool\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfe21c9",
   "metadata": {},
   "source": [
    "### Run the deployed flow\n",
    "\n",
    "We can go to the UI, click on the recently deployed flow and select ```quick run```\n",
    "\n",
    "![title](images/run-deployment.png)\n",
    "\n",
    "*Important:* When running the deployment, Prefect will create a ```tmp/``` directory and clone the repo from GitHub: \n",
    "\n",
    "1. Make sure that the ```orchestrate.py``` is updated in the repo, as it will use this remote file. \n",
    "2. The current directory when running the deployment is the root directory ```mlops-zoomcamp```. Thus make sure to update your path to read the data accordingly.\n",
    "3. Finally, the ```.parquet``` files must be uploaded to GitHub or other remote location "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f85375",
   "metadata": {},
   "source": [
    "## 3.5  Working with Deployments\n",
    "\n",
    "### Setting up multiple deployments\n",
    "\n",
    "We can set up multiple deployments from the project by modifying the ```deployment.yaml``` file:\n",
    "\n",
    "```\n",
    "deployments:\n",
    "- name: taxi_local_data1\n",
    "  flow_name: cohorts/2023/03-orchestration/3.4/orchestrate.py:main_flow\n",
    "  work_pool: zoompool\n",
    "    name: zoompool\n",
    "- name: taxi_local_data2\n",
    "  flow_name: cohorts/2023/03-orchestration/3.5/orchestrate.py:main_flow_35\n",
    "  work_pool: zoompool\n",
    "    name: zoompool\n",
    "```\n",
    "\n",
    "where we have set up two deployments (one for from the script in 3.4 and other for 3.5). Now we call call these two deployments by using:\n",
    "\n",
    "``` $ prefect deploy --all ```\n",
    "\n",
    "![title](images/multiple-deployment.png)\n",
    "\n",
    "In the next section we will create a markdown artifact and then run one of the two deployments above, ```taxi_local_data2```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f319969",
   "metadata": {},
   "source": [
    "### Creating a Markdown artifact\n",
    "\n",
    "We will add the following code at the end of function ```train_best_model``` in ```3.5/orchestrate.py```.\n",
    "\n",
    "```python\n",
    "markdown__rmse_report = f\"\"\"# RMSE Report\n",
    "\n",
    "        ## Summary\n",
    "\n",
    "        Duration Prediction \n",
    "\n",
    "        ## RMSE XGBoost Model\n",
    "\n",
    "        | Region    | RMSE |\n",
    "        |:----------|-------:|\n",
    "        | {date.today()} | {rmse:.2f} |\n",
    "        \"\"\"\n",
    "\n",
    "create_markdown_artifact(\n",
    "      key=\"duration-model-report\", \n",
    "      markdown=markdown__rmse_report\n",
    ")\n",
    "```\n",
    "\n",
    "and make sure that the new code is commited and pushed to the GitHub repo. Then we execute:\n",
    "\n",
    "```$ prefect deployment run main-flow-35/taxi_local_data2 ```\n",
    "\n",
    "to run the first deployment in the ```deployment.yaml``` file (so far we have done this via the UI), and \n",
    "\n",
    "```$ prefect worker start -p zoompool```\n",
    "\n",
    "to have our workers running. We can see the markdown artifact in the UI:\n",
    "\n",
    "![title](images/artifact.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03084c68",
   "metadata": {},
   "source": [
    "### Schedule deployments\n",
    "\n",
    "The deployments can be scheduled to run for a given interval via the UI, CLI or the ```deployment.yaml``` file. For example, using the UI:\n",
    "\n",
    "![title](images/schedule.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17b81b2",
   "metadata": {},
   "source": [
    "## 3.5 Prefect Cloud\n",
    "\n",
    "It allows you to host the server on the cloud, instead of the server being on your computer. In this case the platform is run by the Prefect team. It's a hybrid model:\n",
    "\n",
    "- You run your code on your infrastructure\n",
    "- Data doesn't get sent to Prefect Cloud, just some metadata\n",
    "- Worker opens connection with Cloud and sends metadata\n",
    "\n",
    "![title](images/prefect-cloud.png)\n",
    "\n",
    "You can check your Prefect profile:\n",
    "\n",
    "```$ prefect profile ls```\n",
    "\n",
    "so that you can save different configurations. The information about profiles is kept in ```.prefect/profiles.toml```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c9fe8f",
   "metadata": {},
   "source": [
    "### Authentification\n",
    "\n",
    "Let's get started with Prefect Cloud. First, you need to create an account in [Prefect Cloud](https://app.prefect.cloud) (Prefect Cloud for personal use is free). Once logged in, you enter Workspaces (not present on the local server), where you can share projects and collaborate with other people.\n",
    "\n",
    "![title](images/prefect-cloud-workspaces.png)\n",
    "\n",
    "To get some of the information from the local server (section 3.5) to Prefect Cloud, we should authenticate our local CLI first:\n",
    "\n",
    "```$ prefect cloud login```\n",
    "\n",
    "It will give you the option to authenticate by either:\n",
    "- Web Browser (API key that experies after 30 days)\n",
    "- Paste an API key (previously created by going to your profile in Prefect Cloud)\n",
    "\n",
    "You can verify that your profile is compatible with working with Cloud by running:\n",
    "\n",
    "```$ prefect version```\n",
    "\n",
    "and verifying that the ```Server type``` is set to ```cloud```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c849081b",
   "metadata": {},
   "source": [
    "### Deploying on Prefect Cloud\n",
    "\n",
    "Creating a work pool and assigning a worker:\n",
    "\n",
    "```$ prefect worker start -p zoompool -t process```\n",
    "\n",
    "View in the browser:\n",
    "\n",
    "![title](images/zoompool.png)\n",
    "\n",
    "Now we can deploy the flows from section 3.5 (described in ```deployment.yaml```:\n",
    "\n",
    "```$ prefect deploy --all```\n",
    "\n",
    "View in the browser:\n",
    "\n",
    "![title](images/deployments.png)\n",
    "\n",
    "Finally, we can run the second deployment by going clicking on ```Run```\n",
    "\n",
    "![title](images/run.png)\n",
    "\n",
    "and we would get the usual logs and markdown artifact that we defined in Section 3.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad043c",
   "metadata": {},
   "source": [
    "### Automations\n",
    "\n",
    "You can set up notifications when the Deployments reach some predefined state (e.g. Completed) by using Automations:\n",
    "\n",
    "![title](images/automations.png)\n",
    "\n",
    "You can add an email notification by setting up the ```Block``` tab:\n",
    "\n",
    "![title](images/automations-email.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlops-zoomcamp]",
   "language": "python",
   "name": "conda-env-mlops-zoomcamp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
