{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ea4de0",
   "metadata": {},
   "source": [
    "# Infrastructure as Code with Terraform\n",
    "\n",
    "## 1. Terraform introduction\n",
    "\n",
    "What is Infrastructure as Code with Terraform?\n",
    "\n",
    "Infrastructure as code (IaC) tools allow you to manage infrastructure with configuration files rather than through a graphical user interface. IaC allows you to build, change, and manage your infrastructure in a safe, consistent, and repeatable way by defining resource configurations that you can version, reuse, and share.\n",
    "\n",
    "With Terraform, you describe your desired infrastructure in a declarative language, which looks like plain text files. These files contain the \"code\" that represents your infrastructure blueprint. It's like creating a recipe for your infrastructure.\n",
    "\n",
    "When you run Terraform, it reads these code files and communicates with the cloud provider or infrastructure platform of your choice (like AWS, Azure, Google Cloud, etc.). Terraform then automatically sets up and manages the required resources to match the configuration you defined in your code.\n",
    "\n",
    "### 1.1. Target configuration\n",
    "\n",
    "- Setting up a stream-based pipeline infrastructure in AWS, using Terraform\n",
    "- Project infrastructure modules (AWS): \n",
    "    1. Kinesis Streams (Producer & Consumer)\n",
    "    2. Lambda (Serving API)\n",
    "    3. Cloud Watch event (trigger Lambda whenever an event arrives in the Producer Kinesis stream)\n",
    "    4. S3 Bucket (Model artifacts)\n",
    "    5. ECR (Image Registry)\n",
    "\n",
    "\n",
    "![title](images/AWS-stream-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6f39e5",
   "metadata": {},
   "source": [
    "### 1.2. Install Terraform and AWS configuration\n",
    "\n",
    "1. To install Terraform in Linux follow the steps described in this [link](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli).\n",
    "2. To set up the AWS configuration follow these [steps](https://github.com/DataTalksClub/mlops-zoomcamp/tree/main/06-best-practices/code#iac) (scroll down to IaC section)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa37447",
   "metadata": {},
   "source": [
    "## 2 Terraform: Modules and outputs variables\n",
    "\n",
    "In Terraform, modules are a way to organize and package reusable pieces of infrastructure code. Think of modules as functions in programming â€“ they abstract complex logic and allow you to call them wherever needed, promoting code reusability and maintainability.\n",
    "\n",
    "Modules have input and output variables, which enable them to be flexible and customizable. When you use a module, you can provide input values specific to your use case, and the module returns output values that you can use elsewhere in your Terraform configuration.\n",
    "\n",
    "We create the folder ```code/infrastructure``` with two files:\n",
    "\n",
    "- ```main.tf``` : main configuration file where you define the actual resources (virtual machines, networks, databases, etc)\n",
    "- ```variables.tf```: used to declare input variables that can be used throughout your Terraform configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740aae99",
   "metadata": {},
   "source": [
    "### 2.1 ```main.tf```\n",
    "\n",
    "```python\n",
    "# --> Backend Configuration block\n",
    "terraform {\n",
    "  # Terraform Version Requirement\n",
    "  required_version = \">= 1.0\"\n",
    "  # where Terraform should store its state file (Amazon S3)\n",
    "  backend \"s3\" {\n",
    "    # name of the S3 bucket where the state file will be stored\n",
    "    bucket  = \"tf-state-mlops-zoomcamp\"\n",
    "    # filename for the state file within the S3 bucket\n",
    "    key     = \"mlops-zoomcamp-stg.tfstate\"\n",
    "    # AWS region where the S3 bucket is located\n",
    "    region  = \"eu-west-1\"\n",
    "    # state file is encrypted for better security\n",
    "    encrypt = true\n",
    "  }\n",
    "}\n",
    "\n",
    "# --> AWS Provider Configuration block\n",
    "provider \"aws\" {\n",
    "  # defined in a separate variables.tf\n",
    "  region = var.aws_region\n",
    "}\n",
    "\n",
    "# --> AWS Plugin block\n",
    "# retrieves the current AWS caller ID\n",
    "# The result is stored in a data object called current_identity\n",
    "data \"aws_caller_identity\" \"current_identity\" {}\n",
    "\n",
    "# assigns the AWS account ID to a local variable\n",
    "locals {\n",
    "  # Equivalent to accessing: Class.object_instance.instance_property\n",
    "  account_id = data.aws_caller_identity.current_identity.account_id\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "*Notes*:\n",
    "- You need to create your state S3 bucket manually beforehand (```tf-state-mlops-zoomcamp```), as Terraform won't create it for you \n",
    "- The provider block allows to install plugins from an official library, in this case AWS\n",
    "- Local variables' scope is ```main.tf``` whereas variables in ```variables.tf``` can be used in the context of the entire project\n",
    "- There will be different state files depending on the stages of the project (development, production, CI/CD,...). A suffix will determine the particular stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de495956",
   "metadata": {},
   "source": [
    "### 2.2 ```variables.tf```\n",
    "\n",
    "```python\n",
    "# This block defines a variable called aws_region\n",
    "variable \"aws_region\" {\n",
    "  # brief description of the variable's purpose\n",
    "  description = \"AWS region to create resources\"\n",
    "  # If no value is given to aws_region, it default to \"eu-west-1\"\n",
    "  default     = \"eu-west-1\"\n",
    "}\n",
    "\n",
    "variable \"project_id\" {\n",
    "  description = \"project_id\"\n",
    "  default = \"mlops-zoomcamp\"\n",
    "}\n",
    "\n",
    "# Kinesis input stream\n",
    "variable \"source_stream_name\" {\n",
    "  description = \"\"\n",
    "}\n",
    "\n",
    "# Kinesis output stream\n",
    "variable \"output_stream_name\" {\n",
    "  description = \"\"\n",
    "}\n",
    "\n",
    "variable \"model_bucket\" {\n",
    "  description = \"s3_bucket\"\n",
    "}\n",
    "\n",
    "variable \"lambda_function_local_path\" {\n",
    "  description = \"\"\n",
    "}\n",
    "\n",
    "variable \"docker_image_local_path\" {\n",
    "  description = \"\"\n",
    "}\n",
    "\n",
    "variable \"ecr_repo_name\" {\n",
    "  description = \"\"\n",
    "}\n",
    "\n",
    "variable \"lambda_function_name\" {\n",
    "  description = \"\"\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a4981",
   "metadata": {},
   "source": [
    "### 2.3 Modules\n",
    "\n",
    "Let's create custom modules in ```code/infrastructure/modules``` for:\n",
    "1. Kinesis, ```kinesis/```\n",
    "2. S3 bucket, ```s3/```\n",
    "3. ECR, ```ecr/```\n",
    "4. Lambda, ```lambda/```\n",
    "\n",
    "\n",
    "The Cloud Watch event will be created within Lambda module. \n",
    "\n",
    "### 2.3.1 Kinesis module\n",
    "We will start working on Kinesis by creating two files:\n",
    "1. ```code/infrastructure/modules/kinesis/```\n",
    "    - ```main.tf```\n",
    "        ```python\n",
    "            # --> AWS Kinesis Data Stream resource (server, application,...)\n",
    "            resource \"aws_kinesis_stream\" \"stream\" {\n",
    "              # Variables defined in kinesis/variables.tf\n",
    "              name             = var.stream_name\n",
    "              # number of shards in the Kinesis stream.\n",
    "              shard_count      = var.shard_count\n",
    "              # retention period of data in the Kinesis stream\n",
    "              retention_period = var.retention_period\n",
    "              # shard-level metrics to f monitoring the Kinesis stream \n",
    "              shard_level_metrics = var.shard_level_metrics\n",
    "              # specify tags for the Kinesis stream\n",
    "              tags = {\n",
    "                CreatedBy = var.tags\n",
    "              }\n",
    "            }\n",
    "        \n",
    "            # --> Output variable Block (could have been a separate output.tf)\n",
    "            output \"stream_arn\" {\n",
    "              # exposes the ARN (Amazon Resource Name) of the Kinesis stream \n",
    "              value = aws_kinesis_stream.stream.arn\n",
    "            }\n",
    "\n",
    "        ```\n",
    "    - ```variables.tf```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c140c559",
   "metadata": {},
   "source": [
    "#### Producer stream\n",
    "\n",
    "We can now add the following code to the ```infrastructure/main.tf``` file as shown below:\n",
    "\n",
    "```python\n",
    "# ... previous code\n",
    "\n",
    "# Create Kinesis Data Stream for ride events\n",
    "module \"source_kinesis_stream\" {\n",
    "  source = \"./modules/kinesis\"\n",
    "  retention_period = 48\n",
    "  shard_count = 2\n",
    "  stream_name = \"${var.source_stream_name}-${var.project_id}\"\n",
    "  tags = var.project_id\n",
    "}\n",
    "```\n",
    "You are instantiating the Kinesis module (```infrastructure/modules/kinesis```) to create a Kinesis Data Stream for ride events. The module will be instantiated using the configuration provided in the Kinesis module's ```main.tf``` and ```variables.tf```. If not found there, it will search in the parent directory ```infrastructure/variables.tf```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544d3fb9",
   "metadata": {},
   "source": [
    "Now we can test that everything so far is working by running (from ```code/infrastructure/```):\n",
    "\n",
    "```\n",
    "$ terraform init\n",
    "```\n",
    "which will initialize Terraform by downloading the AWS provider and installing it (in ```.terraform```). Next we run:\n",
    "\n",
    "```\n",
    "$ terraform plan\n",
    "```\n",
    "which describes your execution plan (which actions Terraform will take in order to build the infrastructure to match the configuration). \n",
    "\n",
    "- It will ask for the ```source_stream_name``` value, as it hasn't been configured yet. Give the name ```ride-events```\n",
    "\n",
    "Finally we can run:\n",
    "\n",
    "```\n",
    "$ terraform apply\n",
    "```\n",
    "It will ask for the ```source_stream_name``` again. Now you are generating the actual infrastructure on your AWS cloud. You will need to confirm this action.\n",
    "\n",
    "Now we can confirm that it's created by checking the AWS cloud console.\n",
    "\n",
    "- Note: This infrastructure that we have created will not charge you unless you are using them (they remain idle). However, make sure to destroy your Terraform resources after you are done to avoid any unnecessary costs:\n",
    "\n",
    "```\n",
    "$ terraform destroy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a31efc0",
   "metadata": {},
   "source": [
    "#### Consumer stream\n",
    "\n",
    "We add the following code to the ```infrastructure/main.tf```:\n",
    "\n",
    "```python\n",
    "\n",
    "# ... previous code\n",
    "\n",
    "# Producer stream\n",
    "# ...\n",
    "\n",
    "# Create Kinesis Data Stream for ride predictions\n",
    "module \"output_kinesis_stream\" {\n",
    "  source = \"./modules/kinesis\"\n",
    "  retention_period = 48\n",
    "  shard_count = 2\n",
    "  stream_name = \"${var.output_stream_name}-${var.project_id}\"\n",
    "  tags = var.project_id\n",
    "}\n",
    "```\n",
    "You are instantiating another Kinesis module (infrastructure/modules/kinesis) to create a Kinesis Data Stream for ride predicitons. We have a pair of Kinesis streams now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a515791b",
   "metadata": {},
   "source": [
    "### 2.3.2. S3 module\n",
    "\n",
    "We will now create a S3 bucket for model artifacts:\n",
    "\n",
    "2. ```code/infrastructure/modules/s3/```\n",
    "    - ```main.tf```\n",
    "        ```python\n",
    "           resource \"aws_s3_bucket\" \"s3_bucket\" {\n",
    "             bucket = var.bucket_name\n",
    "             # only the bucket owner has access to the bucket\n",
    "             acl    = \"private\"\n",
    "             # bucket can be destroyed even if it contains objects\n",
    "             force_destroy = true\n",
    "           }\n",
    "           \n",
    "           # bucket name to be exposed as output\n",
    "           output \"name\" {\n",
    "             value = aws_s3_bucket.s3_bucket.bucket\n",
    "           }\n",
    "        ```\n",
    "    - ```variables.tf```\n",
    "\n",
    "and update ```infrastructure/main.tf```:\n",
    "\n",
    "```python\n",
    "\n",
    "# ... previous code\n",
    "\n",
    "# Producer stream\n",
    "# ...\n",
    "\n",
    "# Consumer stream\n",
    "# ...\n",
    "\n",
    "# model bucket\n",
    "module \"s3_bucket\" {\n",
    "  source = \"./modules/s3\"\n",
    "  bucket_name = \"${var.model_bucket}-${var.project_id}\"\n",
    "}\n",
    "```\n",
    "\n",
    "Now you are instantiating the S3 bucket module (```./modules/s3```) to create an AWS S3 bucket for storing models. \n",
    "\n",
    "- Note: this S3 is created **during** our Terraform project. It's not the same S3 bucket that we created in the beginning to store our Terraform state.\n",
    "\n",
    "When executing Terraform (as shown above), it will ask for a value for ```s3_bucket```. You can give the name ```mlflow-models```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e75f4",
   "metadata": {},
   "source": [
    "### 2.3.3 ECR module\n",
    "\n",
    "Terraform builds all modules in parallel. However, we want to create our Kinesis streams, S3 bucket, and ECR (not implemented yet) before Lambda. Otherwise Lambda may not find\n",
    "- the S3 bucket to take the model from\n",
    "- the Kinesis stream to take the source event from.\n",
    "- the docker image to run on (from the ECR registry)\n",
    "\n",
    "Thus, while building the docker image registry (ECR) module:\n",
    "\n",
    "3. ```code/infrastructure/modules/ecr/```\n",
    "    - ```main.tf```\n",
    "        ```python\n",
    "           # --> AWS Elastic Container Registry (ECR) repository resource\n",
    "           resource \"aws_ecr_repository\" \"repo\" {\n",
    "              name                 = var.ecr_repo_name\n",
    "              # image tags can be updated or overwritten\n",
    "              image_tag_mutability = \"MUTABLE\"\n",
    "              # image scanning will not be performed automatically when \n",
    "              # new images are pushed to the repository\n",
    "              image_scanning_configuration {\n",
    "                scan_on_push = false\n",
    "              }\n",
    "              # ECR repository to be destroyed even if it contains images\n",
    "              force_delete = true\n",
    "            }\n",
    "        \n",
    "            # In practice, the Image build-and-push step is handled\n",
    "            # separately by the CI/CD pipeline and not the IaC script.\n",
    "            # But because the lambda config would fail without an\n",
    "            # existing Image URI in ECR, we can also upload any base\n",
    "            # image to bootstrapthe lambda config, unrelated to your\n",
    "            # Inference logic\n",
    "            # --> The null_resource ensures that the Docker image is available\n",
    "            # --> in ECR before the rest of the infrastructure, such as the \n",
    "            # --> Lambda function, is provisioned, preventing errors due to \n",
    "            # --> missing image URIs in ECR.\n",
    "            resource null_resource ecr_image {\n",
    "               # The Docker image will be rebuilt and pushed to ECR when\n",
    "               # the associated files change\n",
    "               triggers = {\n",
    "                 python_file = md5(file(var.lambda_function_local_path))\n",
    "                 docker_file = md5(file(var.docker_image_local_path))\n",
    "               }\n",
    "               # Executes a local shell command when this null_resource \n",
    "               # is created or updated\n",
    "               provisioner \"local-exec\" {\n",
    "                 # multi-line shell script\n",
    "                 command = <<EOF\n",
    "                         # logs into the ECR repository\n",
    "                         aws ecr get-login-password # ... \n",
    "                             | docker login # ...\n",
    "                         # Changes the working directory to parent directory \n",
    "                         cd ../\n",
    "                         # builds a Docker image\n",
    "                         docker build -t # ...\n",
    "                         # Pushes the Docker image to the ECR repository\n",
    "                         docker push # ...\n",
    "                     EOF\n",
    "               }\n",
    "            }\n",
    "\n",
    "            # This block creates the lambda_image (Lambda) to retrieve information\n",
    "            # about the ECR image\n",
    "            data aws_ecr_image lambda_image {\n",
    "             # Terraform waits for the image to be uploaded to ECR  \n",
    "             # before lambda config runs.\n",
    "             depends_on = [\n",
    "               null_resource.ecr_image\n",
    "             ]\n",
    "             repository_name = var.ecr_repo_name\n",
    "             image_tag       = var.ecr_image_tag\n",
    "            }\n",
    "            \n",
    "            # output that exposes the full URI of the uploaded ECR image\n",
    "            output \"image_uri\" {\n",
    "              value     =  # ...\n",
    "            }\n",
    "            \n",
    "        ```       \n",
    "            \n",
    "    - ```variables.tf```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35b1c52",
   "metadata": {},
   "source": [
    "- Notes : the ```null_resource``` is using the Dockerfile and ```lambda_function.py``` defined in ```Part-B/code/```\n",
    "\n",
    "Now we can update our baseline ```infrastructure/main.tf```:\n",
    "\n",
    "```python\n",
    "\n",
    "# ... previous code\n",
    "\n",
    "# Producer stream\n",
    "# ...\n",
    "\n",
    "# Consumer stream\n",
    "# ...\n",
    "\n",
    "# S3 bucket\n",
    "# ...\n",
    "\n",
    "# image registry\n",
    "module \"ecr_image\" {\n",
    "   source = \"./modules/ecr\"\n",
    "   ecr_repo_name = \"${var.ecr_repo_name}_${var.project_id}\"\n",
    "   account_id = local.account_id\n",
    "   lambda_function_local_path = var.lambda_function_local_path\n",
    "   docker_image_local_path = var.docker_image_local_path\n",
    "}\n",
    "```\n",
    "\n",
    "You are instantiating the ECR image module (```./modules/ecr```) to create an AWS ECR repository and push a Docker image to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b65264",
   "metadata": {},
   "source": [
    "#### Create environment variables for runtime\n",
    "\n",
    "Now when executing Terraform many runtime variables are required (especially due to the Docker commands that ECR brings). Instead of introducing them manually, we will create environment variables for runtime:\n",
    "\n",
    "   1. Create a new directory ```infrastructure/vars```\n",
    "   2. We create two files\n",
    "       - Stage : ```stg.tfvars```\n",
    "       - Production: ```prod.tfvars```\n",
    "   3. These files will hold the environment variables for stage and production phases that are required during runtime\n",
    "   4. Now when executing Terraform from ```code/infrastructure/```:\n",
    "        - ```$ terraform init```\n",
    "        - ```$ terraform plan -var-file=vars/stg.tfvars```\n",
    "        - ```$ terraform apply -var-file=vars/stg.tfvars```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79008431",
   "metadata": {},
   "source": [
    "### 2.3.4 Lambda module\n",
    "\n",
    "We now build the Lambda module:\n",
    "\n",
    "4. ```code/infrastructure/modules/lambda/```\n",
    "    - ```main.tf```\n",
    "        ```python\n",
    "            # This block creates an AWS Lambda function resource\n",
    "            resource \"aws_lambda_function\" \"kinesis_lambda\" {\n",
    "              function_name = var.lambda_function_name\n",
    "              # This can also be any base image to bootstrap the lambda config,\n",
    "              # unrelated to your Inference service on ECR\n",
    "              # which would be anyway updated regularly via a CI/CD pipeline\n",
    "                \n",
    "              # Docker image URI to be used for the Lambda function\n",
    "              image_uri     = var.image_uri   # required-argument\n",
    "              # indicate that the Lambda function will use a container image\n",
    "              package_type  = \"Image\"\n",
    "              # defined in 'iam.tf'\n",
    "              role          = aws_iam_role.iam_lambda.arn\n",
    "              # enable active tracing for the Lambda function\n",
    "              tracing_config {\n",
    "                mode = \"Active\"\n",
    "              }\n",
    "              # Define environment variables for the Lambda function (optional)\n",
    "              environment {\n",
    "                variables = {\n",
    "                  PREDICTIONS_STREAM_NAME = var.output_stream_name\n",
    "                  MODEL_BUCKET = var.model_bucket\n",
    "                }\n",
    "              }\n",
    "              # Max time (in seconds) to evaluate Lambda\n",
    "              timeout = 180\n",
    "            }\n",
    "\n",
    "            # AWS Lambda event invoke configuration resource\n",
    "            resource \"aws_lambda_function_event_invoke_config\" \"kinesis_lambda_event\" {\n",
    "              function_name = aws_lambda_function.kinesis_lambda.function_name\n",
    "              # maximum age of an event (in seconds) that the Lambda \n",
    "              # function can invoke\n",
    "              maximum_event_age_in_seconds = 60\n",
    "              # there should be no retries if the Lambda function invocation fails\n",
    "              maximum_retry_attempts       = 0\n",
    "            }\n",
    "            \n",
    "            # AWS Lambda event mapping resource (to producer stream)\n",
    "            resource \"aws_lambda_event_source_mapping\" \"kinesis_mapping\" {\n",
    "              # specifies the ARN of the Kinesis stream to which the \n",
    "              # Lambda function will be mapped\n",
    "              event_source_arn  = var.source_stream_arn\n",
    "              function_name     = aws_lambda_function.kinesis_lambda.arn\n",
    "              # Lambda function should start processing records from the most\n",
    "              # recent records in the Kinesis stream\n",
    "              starting_position = \"LATEST\"\n",
    "              # This ensures that the necessary IAM role policy attachment \n",
    "              # is in place before creating the Lambda event source mapping\n",
    "              depends_on = [\n",
    "                aws_iam_role_policy_attachment.kinesis_processing\n",
    "              ]\n",
    "              # enabled           = var.lambda_event_source_mapping_enabled\n",
    "              # batch_size        = var.lambda_event_source_mapping_batch_size\n",
    "            }\n",
    "        ```\n",
    "     - ```variables.tf```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5e9ab1",
   "metadata": {},
   "source": [
    "Now we can update our baseline ```infrastructure/main.tf```:\n",
    "\n",
    "```python\n",
    "\n",
    "# ... previous code\n",
    "\n",
    "# Producer stream\n",
    "# ...\n",
    "\n",
    "# Consumer stream\n",
    "# ...\n",
    "\n",
    "# S3 bucket\n",
    "# ...\n",
    "\n",
    "# ECR\n",
    "# ...\n",
    "\n",
    "module \"lambda_function\" {\n",
    "  source = \"./modules/lambda\"\n",
    "  # This specifies the Docker container image URI stored in the Elastic\n",
    "  # Container Registry (ECR). The Lambda function will use this image \n",
    "  # to run the code.\n",
    "  image_uri = module.ecr_image.image_uri\n",
    "  # creates a unique name for the Lambda function\n",
    "  lambda_function_name = \"${var.lambda_function_name}_${var.project_id}\"\n",
    "  # It specifies the name of the S3 bucket where the Lambda function can\n",
    "  # access the machine learning model.\n",
    "  model_bucket = module.s3_bucket.name\n",
    "  # creates a unique name for the output Kinesis stream \n",
    "  output_stream_name = \"${var.output_stream_name}-${var.project_id}\"\n",
    "  # specifies the Amazon Resource Name (ARN) of the output Kinesis stream\n",
    "  output_stream_arn = module.output_kinesis_stream.stream_arn\n",
    "  source_stream_name = \"${var.source_stream_name}-${var.project_id}\"\n",
    "  source_stream_arn = module.source_kinesis_stream.stream_arn\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "You are defining an AWS Lambda function to consume data from a Kinesis stream using an ECR container image, as well as the necessary event source mapping to connect the Lambda function with the Kinesis stream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3567a8",
   "metadata": {},
   "source": [
    "#### Configuring Lambda dependencies\n",
    "\n",
    "The lambda module has a lot of dependencies wrt Kinesis, S3 and ECR. Therefore, we need to define various AWS Identity and Access Management (IAM) resources required for the Lambda function to interact with other AWS services such as Kinesis, CloudWatch, and S3. This ensures that your Lambda function has the necessary access to perform its intended operations on these services:\n",
    "\n",
    "1. Create ```lambda/iam.tf``` file.\n",
    "    - The first blocks connect lambda with Kinesis:\n",
    "    ```python\n",
    "       # --> This block creates an AWS IAM Role Resource\n",
    "       resource \"aws_iam_role\" \"iam_lambda\" {\n",
    "          # Creates a unique IAM role name based on the Lambda\n",
    "          name = \"iam_${var.lambda_function_name}\"\n",
    "          # defines which AWS services are allowed to assume this role.\n",
    "          # In this case, it allows both Lambda and Kinesis services to\n",
    "          # assume the role.\n",
    "          assume_role_policy = \n",
    "          # ...\n",
    "        }\n",
    "        \n",
    "        # --> This block creates an IAM policy resource for Kinesis\n",
    "        resource \"aws_iam_policy\" \"allow_kinesis_processing\" {\n",
    "          # Allows Kinesis processing based on the Lambda function's name\n",
    "          name        = \"allow_kinesis_processing_${var.lambda_function_name}\"\n",
    "          path        = \"/\"\n",
    "          description = \"IAM policy for logging from a lambda\"\n",
    "          \n",
    "          # Specifies the permissions granted to the IAM role related \n",
    "          # to Kinesis. In this case, it allows various Kinesis-related\n",
    "          # actions on all Kinesis resources\n",
    "          policy = \n",
    "            # ...\n",
    "        }\n",
    "        \n",
    "        # --> This block grants the IAM role the permissions defined in the \n",
    "        #     IAM policy\n",
    "        resource \"aws_iam_role_policy_attachment\" \"kinesis_processing\" {\n",
    "          role       = aws_iam_role.iam_lambda.name\n",
    "          policy_arn = aws_iam_policy.allow_kinesis_processing.arn\n",
    "        }\n",
    "        \n",
    "        # --> This block creates an inline IAM policy resource\n",
    "        resource \"aws_iam_role_policy\" \"inline_lambda_policy\" {\n",
    "          name       = \"LambdaInlinePolicy\"\n",
    "          role       = aws_iam_role.iam_lambda.id\n",
    "          # ensures that this policy is created after the IAM role\n",
    "          depends_on = [aws_iam_role.iam_lambda]\n",
    "          # defines the inline policy's permissions. In this case, it grants\n",
    "          # the Lambda function permission to put records to the Kinesis stream\n",
    "          policy     = \n",
    "            # ...\n",
    "        }\n",
    "        \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c5059a",
   "metadata": {},
   "source": [
    "- The next blocks set up CloudWatch logging:\n",
    "\n",
    "```python\n",
    "        # --> This block creates an AWS Lambda permission for CloudWatch\n",
    "        #     (to trigger Lambda)\n",
    "        resource \"aws_lambda_permission\" \"allow_cloudwatch_to_trigger_lambda_function\" \n",
    "        {\n",
    "          statement_id  = \"AllowExecutionFromCloudWatch\"\n",
    "          action        = \"lambda:InvokeFunction\"\n",
    "          function_name = aws_lambda_function.kinesis_lambda.function_name\n",
    "          # indicates that the permission is granted to the CloudWatch \n",
    "          # Events service.\n",
    "          principal     = \"events.amazonaws.com\"\n",
    "          # This attribute specifies the ARN of the event source (Kinesis stream)\n",
    "          # that triggers the Lambda function.\n",
    "          source_arn    = var.source_stream_arn\n",
    "        }\n",
    "        \n",
    "        # --> This block creates an IAM policy for Logging\n",
    "        resource \"aws_iam_policy\" \"allow_logging\" {\n",
    "          name        = \"allow_logging_${var.lambda_function_name}\"\n",
    "          path        = \"/\"\n",
    "          description = \"IAM policy for logging from a lambda\"\n",
    "          # specifies the permissions granted to the IAM role for logging\n",
    "          # purposes. In this case, it allows actions related to CloudWatch Logs\n",
    "          policy = \n",
    "            # ...\n",
    "        }\n",
    "        \n",
    "        # --> This block grants the IAM role the permissions defined \n",
    "        #     in the IAM policy for logging\n",
    "        resource \"aws_iam_role_policy_attachment\" \"lambda_logs\" {\n",
    "          role       = aws_iam_role.iam_lambda.name\n",
    "          policy_arn = aws_iam_policy.allow_logging.arn\n",
    "        }\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42db3085",
   "metadata": {},
   "source": [
    "- And finally permissions for the S3 bucket\n",
    "\n",
    "```python\n",
    "        \n",
    "        # --> This block creates an IAM policy for S3\n",
    "        resource \"aws_iam_policy\" \"lambda_s3_role_policy\" {\n",
    "          name = \"lambda_s3_policy_${var.lambda_function_name}\"\n",
    "          description = \"IAM Policy for s3\"\n",
    "          # it allows various S3-related actions, including listing \n",
    "          # buckets, accessing bucket locations, and performing actions\n",
    "          # on the specified s3 bucket and its contents\n",
    "        policy = \n",
    "          # ...\n",
    "        }\n",
    "        \n",
    "        # --> This block grants the IAM role the permissions defined \n",
    "        #     in the IAM policy for S3 actions.\n",
    "        resource \"aws_iam_role_policy_attachment\" \"iam-policy-attach\" {\n",
    "          role       = aws_iam_role.iam_lambda.name\n",
    "          policy_arn = aws_iam_policy.lambda_s3_role_policy.arn\n",
    "        }\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f4bc0a",
   "metadata": {},
   "source": [
    "## 3 Terraform: putting everything together\n",
    "\n",
    "![title](images/terraform.png)\n",
    "\n",
    "Now we have the entire pipeline deployed on AWS using Terraform. Let's try to execute the entire infrastructure:\n",
    "\n",
    "- Insert ride event record into our input Kinesis stream (Producer stream)\n",
    "- This will trigger the Lambda service\n",
    "- A version of the ML model (artifact) will be taken from s3 bucket\n",
    "- Ride predictions (from the logic of the docker image in ECR) will be generated \n",
    "- Publish these predictions into the output Kinesis stream (consumer stream)\n",
    "\n",
    "Therefore (from ```code/infrastructure```):\n",
    "\n",
    "1. Create infrastructure\n",
    "```bash\n",
    "# Initialize state file (.tfstate)\n",
    "$ terraform init\n",
    "```\n",
    "```bash\n",
    "# Check changes to new infra plan\n",
    "$ terraform plan -var-file=vars/stg.tfvars\n",
    "```\n",
    "```bash\n",
    "# Create new infra\n",
    "$ terraform apply -var-file=vars/stg.tfvars\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63012780",
   "metadata": {},
   "source": [
    "2. Additionally, we need to define some environmental variables for Lambda:\n",
    "\n",
    "    ![title](images/clip.png)\n",
    "    \n",
    "    - Output Kinesis stream (to write the records)\n",
    "    - S3 bucket to take the model artifacts from\n",
    "    - Run ID (the version of the model it's supposed to take)\n",
    "    \n",
    "\n",
    "   The script taking care of this is ```code/scripts/deploy_manual.sh```\n",
    "\n",
    "   We can now run the script:\n",
    "\n",
    "   ```bash\n",
    "   $ ./deploy_manual.sh\n",
    "   ```\n",
    "It's copying the artifacts into our production bucket and updating the Lambda with the aforementioned env variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5aee4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "3. Now let's try to put a record into our Kinesis stream: \n",
    "![title](images/clip2.png)\n",
    "```bash\n",
    "$ export KINESIS_STREAM_INPUT=\"stg_ride_events-mlops-zoomcamp\"\n",
    "$ aws kinesis put-record  \\\n",
    "        --stream-name ${KINESIS_STREAM_INPUT}   \\\n",
    "        --partition-key 1  --cli-binary-format raw-in-base64-out  \\\n",
    "        --data '{\"ride\": {\n",
    "                    \"PULocationID\": 130,\n",
    "                    \"DOLocationID\": 205,\n",
    "                    \"trip_distance\": 3.66\n",
    "                    },\n",
    "                 \"ride_id\": 156}'\n",
    "```\n",
    "  and you should get a ```ShardId``` and ```SequenceNumber``` as output. You can also check on CloudWatch logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8995782",
   "metadata": {},
   "source": [
    "4. Destroy infrastructure after use (from ```code/infrastructure```)::\n",
    "\n",
    "```bash\n",
    "# Delete infra after your work, to avoid costs on any running services\n",
    "$ terraform destroy\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlops-zoomcamp]",
   "language": "python",
   "name": "conda-env-mlops-zoomcamp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
