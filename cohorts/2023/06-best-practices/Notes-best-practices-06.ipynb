{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ece5619",
   "metadata": {},
   "source": [
    "## 6.1 - Testing Python code with pytest\n",
    "\n",
    "The idea in this section is to perform **unit testing**, that is testing individual pieces of the code.\n",
    "\n",
    "We will take the deployment example from Week 4 (streaming). In this example we used Lambda and Kinesis (AWS). It had this simple architecture:\n",
    "\n",
    "```   \n",
    "                      (4) Model\n",
    "                     (S3 Bucket)\n",
    "                         |\n",
    "    Stream of         Service         Predictions \n",
    "(1) events    ->  (2) (Lambda) -> (3) Stream\n",
    "    (Kinesis)                         (Kinesis)\n",
    "```\n",
    "\n",
    "1. We have the stream of events\n",
    "2. The service w/ model (from an S3 bucket (4)) reads and reacts to these events \n",
    "4. The service applies the model to input stream to get the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3973f6e",
   "metadata": {},
   "source": [
    "The development will be done in the new folder ```notes-code```, where we had copied the content from ```04-deployment/streaming```:\n",
    "\n",
    "```\n",
    "Dockerfile          Pipfile       model.py\n",
    "lambda_function.py  Pipfile.lock  test_docker.py\n",
    "```\n",
    "\n",
    "where we have removed the original```README.md``` and ```test.py``` files. We then create a virtual environment (we could have just used the conda environment ```mlops-zoomcamp``` used in previous weeks instead):\n",
    "\n",
    "```\n",
    "$ pipenv install boto3 mlflow scikit-learn\n",
    "```\n",
    "\n",
    "and create a test folder ```notes-code/tests``` with ```__init__.py``` so that python knows it can be imported as package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587da8ba",
   "metadata": {},
   "source": [
    "Finally the install pytest as a dev dependency (we do not need it for production):\n",
    "\n",
    "```\n",
    "$ pipenv install --dev pytest\n",
    "```\n",
    "\n",
    "To enter the virtualenv:\n",
    "\n",
    "```\n",
    "$ pipenv shell\n",
    "```\n",
    "\n",
    "We start from the script ```notes-code/lambda_function.py```. We modify (refactor) the original script to make it easier to test:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import model\n",
    "\n",
    "# Get environment variables\n",
    "PREDICTIONS_STREAM_NAME = os.getenv('PREDICTIONS_STREAM_NAME',\n",
    "                                    'ride_predictions')\n",
    "RUN_ID = os.getenv('RUN_ID')\n",
    "TEST_RUN = os.getenv('TEST_RUN', 'False') == 'True'\n",
    "\n",
    "# Initialize the model service\n",
    "model_service = model.init(\n",
    "    prediction_stream_name=PREDICTIONS_STREAM_NAME,\n",
    "    run_id=RUN_ID,\n",
    "    test_run=TEST_RUN,\n",
    ")\n",
    "\n",
    "# Lambda handler function\n",
    "def lambda_handler(event, context):\n",
    "    # pylint: disable=unused-argument\n",
    "    return model_service.lambda_handler(event)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15617181",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- This script serves as the entry point for the AWS Lambda function. It delgates everything to the model service.\n",
    "- It imports the ```model``` service and initializes it using the ```model.init()``` function.\n",
    "- The environment variables PREDICTIONS_STREAM_NAME, RUN_ID, and TEST_RUN are used to configure the model service.\n",
    "- The ```lambda_handler``` function is the main function that AWS Lambda invokes when the function is triggered. It calls the ```lambda_handler``` method of the model service (yes, it's confusing) and passes the event and context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1c9a14",
   "metadata": {},
   "source": [
    "where script ```notes-code/model.py``` is as follows:\n",
    "\n",
    "```python\n",
    "\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "import boto3\n",
    "import mlflow\n",
    "\n",
    "# Function to get the model location (from S3 bucket)\n",
    "def get_model_location(run_id):\n",
    "    # ...\n",
    "\n",
    "# Function to load the MLflow model\n",
    "def load_model(run_id):\n",
    "    # ...\n",
    "\n",
    "# Function to decode base64 data\n",
    "def base64_decode(encoded_data):\n",
    "    # ...\n",
    "\n",
    "# Class representing the model service\n",
    "class ModelService:\n",
    "    # ...\n",
    "\n",
    "# Class representing the Kinesis callback\n",
    "class KinesisCallback:\n",
    "    # ...\n",
    "\n",
    "# Function to create a Kinesis client\n",
    "def create_kinesis_client():\n",
    "    # ...\n",
    "\n",
    "# Function to initialize the model service\n",
    "def init(prediction_stream_name: str, run_id: str, test_run: bool):\n",
    "    # ...\n",
    "\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "\n",
    "- This script contains the main logic for the model service.\n",
    "- It defines functions to get the model from S3 bucket, load the MLflow model, decode base64 data, and create a Kinesis client.\n",
    "- The ```ModelService``` class represents the model service and contains methods for preparing features, making predictions, and handling Lambda events.\n",
    "- The ```KinesisCallback``` class represents a callback for sending prediction events to a Kinesis stream.\n",
    "- The ```init``` function initializes the model service by loading the MLflow model, creating a Kinesis callback (if not in test run), and returning the initialized model service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9686678f",
   "metadata": {},
   "source": [
    "The tests are located in ```notes-code/tests/model_test.py```, which reads as follows:\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "import model\n",
    "\n",
    "# Function to read text from a file\n",
    "def read_text(file):\n",
    "    # ...\n",
    "\n",
    "# Test for base64_decode function\n",
    "def test_base64_decode():\n",
    "    # ...\n",
    "\n",
    "# Test for prepare_features method\n",
    "def test_prepare_features():\n",
    "    # ...\n",
    "\n",
    "# Mock model class for testing predict method\n",
    "class ModelMock:\n",
    "    # ...\n",
    "\n",
    "# Test for predict method\n",
    "def test_predict():\n",
    "    # ...\n",
    "\n",
    "# Test for lambda_handler method\n",
    "def test_lambda_handler():\n",
    "    # ...\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "\n",
    "- This script contains pytest test cases for the model and its functionalities.\n",
    "- The test cases include 4 tests: ```base64_decode``` function, ```prepare_features``` method, ```predict method``` and ```lambda_handler``` method of the ```ModelService``` class.\n",
    "- A ```ModelMock``` class is used to mock the ML model for testing purposes.\n",
    "- Each test case asserts the expected result against the actual result obtained from the tested function or method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642a2599",
   "metadata": {},
   "source": [
    "### 6.1.1 Unit Testing (locally)\n",
    "\n",
    "We can run the tests locally by executing:\n",
    "\n",
    "```\n",
    "$ pipenv run pytest tests/\n",
    "```\n",
    "\n",
    "which will work (with maybe some warnings), as it does not use a model from a S3 bucket but instead it creates a *mock* version of the model only for testing purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20ee80",
   "metadata": {},
   "source": [
    "### 6.1.2 Prelude to Integration Tests (Docker container & connection to AWS)\n",
    "\n",
    "We can then check that the scripts ```lambda_function.py``` and ```model.py``` are working by using the following ```notes-code/Dockerfile```:\n",
    "\n",
    "```dockerfile\n",
    "FROM public.ecr.aws/lambda/python:3.10\n",
    "\n",
    "RUN pip install -U pip\n",
    "RUN pip install pipenv\n",
    "\n",
    "COPY [ \"Pipfile\", \"Pipfile.lock\", \"./\" ]\n",
    "\n",
    "RUN pipenv install --system --deploy\n",
    "\n",
    "COPY [ \"lambda_function.py\", \"model.py\", \"./\" ]\n",
    "\n",
    "CMD [ \"lambda_function.lambda_handler\" ]\n",
    "```\n",
    "\n",
    "Building the docker image:\n",
    "\n",
    "```\n",
    "$ docker build -t stream-model-duration:v2 .\n",
    "```\n",
    "\n",
    "and running the container:\n",
    "\n",
    "```\n",
    "$ docker run -it --rm \\\n",
    "    -p 8080:8080 \\\n",
    "    -e PREDICTIONS_STREAM_NAME=\"ride_predictions\" \\\n",
    "    -e RUN_ID=\"e1efc53e9bd149078b0c12aeaa6365df\" \\\n",
    "    -e TEST_RUN=\"True\" \\\n",
    "    -e AWS_DEFAULT_REGION=\"eu-west-1\" \\\n",
    "    stream-model-duration:v2\n",
    "\n",
    "```\n",
    "\n",
    "Finally, we can test that the container is working by running:\n",
    "\n",
    "```\n",
    "$ python test_docker.py\n",
    "```\n",
    "\n",
    "I haven't configured the AWS account, so it will not work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dcb1e7",
   "metadata": {},
   "source": [
    "## 6.2 - Integration tests with docker-compose\n",
    "\n",
    "**Integration tests** are tests which cover the entire pipeline to assess how well the parts fit together:\n",
    "\n",
    "- It can handle our request\n",
    "- It can decode it\n",
    "- It can download the model from S3 bucket\n",
    "- It can apply the model \n",
    "\n",
    "In section 6.1.1 we have tested the different components separately, but we haven't checked if the entire thing works together. We did a first draft in section 6.1.2. We will continue from there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c114d4d",
   "metadata": {},
   "source": [
    "### 6.2.1 Running (manual) integration test with ```Deepdiff```\n",
    "\n",
    "Thus we start from the script ```test_docker.py``` and turn it into a proper test. The idea is to interface with the Docker container and return a dictionary. We use the ```deepdiff``` library to see the difference between the expected dictionary and the returned dictionary. Therefore, we install the library first:\n",
    "\n",
    "```\n",
    "$ pipenv install --dev deepdiff \n",
    "```\n",
    "\n",
    "Now we rebuild the docker image with: \n",
    "\n",
    "```\n",
    "$ docker build -t stream-model-duration:v2 .\n",
    "```\n",
    "\n",
    "We create a folder called ```notes-code/integration-test``` and copy the ```test_docker.py``` there. We also download the model from S3 into the folder ```integration-test/model```(to have it locally during testing). Once downloaded, the ```model``` folder looks as follows:\n",
    "\n",
    "```\n",
    "conda.yaml  MLmodel  model.pkl  python_env.yaml  requirements.txt\n",
    "```\n",
    "\n",
    "We can now run the container specifying this new model location (run from the ```integration-test``` folder:\n",
    "\n",
    "```\n",
    "$ docker run -it --rm \\\n",
    "    -p 8080:8080 \\\n",
    "    -e PREDICTIONS_STREAM_NAME=\"ride_predictions\" \\\n",
    "    -e RUN_ID=\"Test123\" \\\n",
    "    -e MODEL_LOCATION=\"/app/model\" \\\n",
    "    -e TEST_RUN=\"True\" \\\n",
    "    -e AWS_DEFAULT_REGION=\"eu-west-1\" \\\n",
    "    -v $(pwd)/model:/app/model \\\n",
    "    stream-model-duration:v2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e36fac",
   "metadata": {},
   "source": [
    "### 6.2.2 Automate test with docker-compose\n",
    "\n",
    "So far we have been running the containers manually. We can automate the process by using docker-compose. We can define the bash script ```run.sh```:\n",
    "\n",
    "```bash\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# This command changes the current directory to the directory\n",
    "# where the script resides\n",
    "cd \"$(dirname \"$0\")\"\n",
    "\n",
    "# timestamp format is year-month-day-hour-minute\n",
    "LOCAL_TAG=`date +\"%Y-%m-%d-%H-%M\"`\n",
    "# \"stream-model-duration\" concatenated with the \n",
    "# LOCAL_TAG timestamp. \n",
    "export LOCAL_IMAGE_NAME=\"stream-model-duration:${LOCAL_TAG}\"\n",
    "\n",
    "# Detached mode so next command can be executed\n",
    "docker compose up -d\n",
    "\n",
    "# This line pauses the script execution for 1 second,\n",
    "# allowing Docker Compose services to start.\n",
    "sleep 1\n",
    "\n",
    "pipenv run python test_docker.py\n",
    "\n",
    "# The exit status of the previous command (test_docker.py) \n",
    "# is stored in the ERROR_CODE variable.\n",
    "ERROR_CODE=$?\n",
    "\n",
    "# If the exit status of the previous command is not equal to 0 \n",
    "# (indicating an error occurred), the \"docker compose logs\" \n",
    "# command is executed, which shows the logs of the Docker \n",
    "# Compose services.\n",
    "if [ ${ERROR_CODE} != 0 ]; then\n",
    "    docker compose logs\n",
    "fi\n",
    "\n",
    "docker compose down\n",
    "\n",
    "# The script exits with the same exit code as the previous \n",
    "# command, allowing the caller to know if an error occurred.\n",
    "exit ${ERROR_CODE}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190b96dd",
   "metadata": {},
   "source": [
    "and the ```compose.yaml```:\n",
    "\n",
    "```compose.yaml\n",
    "services:\n",
    "  backend:\n",
    "    image: ${LOCAL_IMAGE_NAME}\n",
    "    ports:\n",
    "      - \"8080:8080\"\n",
    "    environment:\n",
    "      - PREDICTIONS_STREAM_NAME=ride_predictions\n",
    "      - TEST_RUN=True\n",
    "      - RUN_ID=Test123\n",
    "      - AWS_DEFAULT_REGION=eu-west-1\n",
    "      - MODEL_LOCATION=/app/model\n",
    "    volumes:\n",
    "      - \"./model:/app/model\"\n",
    "```\n",
    "\n",
    "Now we can execute the bash script from ```notes-code/``` by running:\n",
    "\n",
    "```\n",
    "$ ./integration-test/run.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de2a953",
   "metadata": {},
   "source": [
    "## 6.3 - Testing cloud services with LocalStack\n",
    "\n",
    "In this section we will test AWS services locally using LocalStack (w/o requiring an AWS account). It's very useful to test Kinesis, S3,...etc.\n",
    "\n",
    "### 6.3.1 Add LocalStack to docker-compose\n",
    "\n",
    "In the previous sections we haven't tested Kinesis. Here we'll use LocalStack to test it. To do that we add the Kinesis Service to the previous ```compose.yaml``` as follows:\n",
    "\n",
    "```compose.yaml\n",
    "  # ...\n",
    "  kinesis:\n",
    "    image: localstack/localstack\n",
    "    ports:\n",
    "      - \"4566:4566\"\n",
    "    environment:\n",
    "      - SERVICES=kinesis\n",
    "```\n",
    "\n",
    "and now we can lunch the container with only the Kinesis Service:\n",
    "\n",
    "```\n",
    "$ docker compose up kinesis\n",
    "```\n",
    "where we have provided a ```integration-test/.env``` file to define environment variables for the Docker Compose setup. We have added a \n",
    " dummy value for ```LOCAL_IMAGE_NAME```, otherwise docker compose will not run. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663164e0",
   "metadata": {},
   "source": [
    "### 6.3.2 ```awscli``` & LocalStack  \n",
    "\n",
    "Test it with \n",
    "```\n",
    "$ aws --endpoint-url=http://localhost:4566 kinesis list-streams \n",
    "```\n",
    "the result should be ```[ ]``` because we haven't created any stream. The command above is using the AWS Command Line Interface (CLI) to interact with a local instance of LocalStack running as a Docker container (specified as ```--endpoint-url=http://localhost:4566```). Make sure that you have ```awscli``` installed and keys and password set:\n",
    "\n",
    " - AWS Access Key ID: ```abc```\n",
    " - AWS Secret Access Key: ```xyz```\n",
    " - Default region name: ```eu-west-1```\n",
    "\n",
    "Let us create a stream. All of this happens locally and no development is made in the AWS account.\n",
    "\n",
    "```\n",
    "$ aws --endpoint-url=http://localhost:4566 \\\n",
    "      kinesis create-stream                \\\n",
    "      --stream-name ride_predictions       \\\n",
    "      --shard-count 1\n",
    "```\n",
    "\n",
    "By executing this command, you are creating a Kinesis stream named *ride_predictions* on your local instance of LocalStack. A stream is a sequence of data records, and shards are the partitions within a stream that allow for parallel processing of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64206cfb",
   "metadata": {},
   "source": [
    "### 6.3.3 Testing Kinesis client\n",
    "\n",
    "We can automate the ```--endpoint-url``` in ```compose.yaml```by adding the line ```KINESIS_ENDPOINT_URL=http://kinesis:4566/```.\n",
    "\n",
    "Next step is to add a function to ```model.py``` to test the Kinesis client:\n",
    "\n",
    "```python\n",
    "def create_kinesis_client():\n",
    "    endpoint_url = os.getenv('KINESIS_ENDPOINT_URL')\n",
    "\n",
    "    if endpoint_url is None:\n",
    "        return boto3.client('kinesis')\n",
    "\n",
    "    return boto3.client('kinesis', endpoint_url=endpoint_url)\n",
    "```\n",
    "\n",
    "which will connect to LocalStack or AWS, depending on the value of ```KINESIS_ENDPOINT_URL```.\n",
    "\n",
    "We then modify ```run.sh``` to add a Kinesis stream every time the integration test is run:\n",
    "\n",
    "```bash\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "cd \"$(dirname \"$0\")\"\n",
    "\n",
    "\n",
    "LOCAL_TAG=`date +\"%Y-%m-%d-%H-%M\"`\n",
    "export LOCAL_IMAGE_NAME=\"stream-model-duration:${LOCAL_TAG}\"\n",
    "export PREDICTIONS_STREAM_NAME='ride_predictions'\n",
    "\n",
    "docker build -t ${LOCAL_IMAGE_NAME} ..\n",
    "\n",
    "docker compose up -d\n",
    "\n",
    "sleep 1\n",
    "\n",
    "aws --endpoint-url=http://localhost:4566       \\\n",
    "      kinesis create-stream                    \\\n",
    "      --stream-name ${PREDICTIONS_STREAM_NAME} \\\n",
    "      --shard-count 1\n",
    "\n",
    "pipenv run python test_docker.py\n",
    "\n",
    "ERROR_CODE=$?\n",
    "\n",
    "if [ ${ERROR_CODE} != 0 ]; then\n",
    "    docker compose logs\n",
    "fi\n",
    "\n",
    "#docker compose down\n",
    "\n",
    "#exit ${ERROR_CODE}\n",
    "```\n",
    "\n",
    "where ```compose.yaml```:\n",
    "\n",
    "```compose.yaml\n",
    "services:\n",
    "  backend:\n",
    "    image: ${LOCAL_IMAGE_NAME}\n",
    "    ports:\n",
    "      - \"8080:8080\"\n",
    "    environment:\n",
    "      - PREDICTIONS_STREAM_NAME=${PREDICTIONS_STREAM_NAME}\n",
    "      # - TEST_RUN=True\n",
    "      - RUN_ID=Test123\n",
    "      - AWS_DEFAULT_REGION=eu-west-1\n",
    "      - MODEL_LOCATION=/app/model\n",
    "      - KINESIS_ENDPOINT_URL=http://kinesis:4566/\n",
    "    volumes:\n",
    "      - \"./model:/app/model\"\n",
    "  kinesis:\n",
    "    image: localstack/localstack\n",
    "    ports:\n",
    "      - \"4566:4566\"\n",
    "    environment:\n",
    "      - SERVICES=kinesis   \n",
    "```\n",
    "\n",
    "And now we can run \n",
    "\n",
    "```\n",
    "$ ./run.sh\n",
    "```\n",
    "and we observe both containers running:\n",
    "- localstack/localstack\n",
    "- stream-model-duration:2023-07-13-13-06\n",
    "\n",
    "We could also list the streams and see the new stream created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b71734",
   "metadata": {},
   "source": [
    "We can obtain the shard-iterator with:\n",
    "\n",
    "```\n",
    "$ aws kinesis get-shard-iterator \\\n",
    "--shard-id shardId-000000000000 \\\n",
    "--shard-iterator-type TRIM_HORIZON \\\n",
    "--stream-name ride_predictions \\\n",
    "--query 'ShardIterator'\n",
    "```\n",
    "\n",
    "and now we can use this shard iterator to see what is inside the Kinesis stream:\n",
    "\n",
    "```\n",
    "$ aws --endpoint-url=http://localhost:4566 kinesis get-records --shard-iterator AAAAAAAAAAEKI4NpH/+OBt9nuDiWveLMU3AC04xCuNo+FAd4A8AG0xie44BvI515xlgURUqDa4yQNbbebn/Mh43NjDCW6tJ8aD87X9PTooaZWjpWklDFXATaLHKT3f+lZSyrsNC8dkb7sS/uLQHyb5OrMKM8YS7kj+LqrX93tZ3hRRaiTavCLF2HYvDA5opnP8sM3/y/dciH2NWrE4PrT4YHoJXSoknd \n",
    "```\n",
    "\n",
    "You can decode the results by using:\n",
    "\n",
    "```\n",
    "$ echo $DATA | base64 -d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606bef49",
   "metadata": {},
   "source": [
    "### 6.3.4 Testing Kinesis client automatically\n",
    "\n",
    "We will use the script ```test_kinesis.py``` to perform all the previous steps automatically. Overall, this script retrieves a single record from an Kinesis stream, compares it to an expected record, and performs assertions to ensure the records match. It uses ```boto3``` for interacting with Kinesis.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import boto3\n",
    "from deepdiff import DeepDiff\n",
    "\n",
    "# If the environment variable is not set (normally set to AWS),\n",
    "# it defaults to LocalStack,\n",
    "kinesis_endpoint = os.getenv('KINESIS_ENDPOINT_URL',\n",
    "                             \"http://localhost:4566\")\n",
    "# Kinesis client using the boto3.client method\n",
    "kinesis_client = boto3.client('kinesis', \n",
    "                              endpoint_url=kinesis_endpoint)\n",
    "\n",
    "\n",
    "stream_name = os.getenv('PREDICTIONS_STREAM_NAME', \n",
    "                        'ride_predictions')\n",
    "# represents a specific shard in the Kinesis stream\n",
    "shard_id = 'shardId-000000000000'\n",
    "\n",
    "# retrieve a shard iterator from Kinesis client\n",
    "# 'TRIM_HORIZON' -> it starts reading from the oldest \n",
    "#                   available records in the shard.\n",
    "shard_iterator_response = kinesis_client.get_shard_iterator(\n",
    "    StreamName=stream_name,\n",
    "    ShardId=shard_id,\n",
    "    ShardIteratorType='TRIM_HORIZON',\n",
    ")\n",
    "\n",
    "# retrieves the actual shard iterator ID\n",
    "shard_iterator_id = shard_iterator_response['ShardIterator']\n",
    "\n",
    "# retrieve records from the stream. It provides the ShardIterator \n",
    "# obtained earlier and sets the Limit parameter to 1, indicating \n",
    "# that only one record should be retrieved.\n",
    "records_response = kinesis_client.get_records(\n",
    "    ShardIterator=shard_iterator_id,\n",
    "    Limit=1,\n",
    ")\n",
    "\n",
    "# retrieves the actual records from the response\n",
    "records = records_response['Records']\n",
    "pprint(records)\n",
    "\n",
    "assert len(records) == 1\n",
    "\n",
    "# retrieves the data field from the first record in the records \n",
    "# list, which contains the actual record as a JSON string.\n",
    "actual_record = json.loads(records[0]['Data'])\n",
    "pprint(actual_record)\n",
    "\n",
    "expected_record = {\n",
    "    'model': 'ride_duration_prediction_model',\n",
    "    'version': 'Test123',\n",
    "    'prediction': {\n",
    "        'ride_duration': 21.3,\n",
    "        'ride_id': 256,\n",
    "    },\n",
    "}\n",
    "\n",
    "diff = DeepDiff(actual_record, \n",
    "                expected_record,\n",
    "                significant_digits=1)\n",
    "print(f'diff={diff}')\n",
    "\n",
    "# These lines assert that there are no differences related to \n",
    "# changed values or type changes between the actual_record and \n",
    "# expected_record. If any such differences exist, an exception\n",
    "# will be raised.\n",
    "assert 'values_changed' not in diff\n",
    "assert 'type_changes' not in diff\n",
    "\n",
    "# If the script reaches this line, it means that no assertion \n",
    "# errors occurred, and the comparison between the actual and \n",
    "# expected records passed successfully.\n",
    "print('all good')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27edfc7",
   "metadata": {},
   "source": [
    "and ```run.sh``` is modified to:\n",
    "\n",
    "```bash\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "cd \"$(dirname \"$0\")\"\n",
    "\n",
    "\n",
    "LOCAL_TAG=`date +\"%Y-%m-%d-%H-%M\"`\n",
    "export LOCAL_IMAGE_NAME=\"stream-model-duration:${LOCAL_TAG}\"\n",
    "export PREDICTIONS_STREAM_NAME='ride_predictions'\n",
    "\n",
    "docker build -t ${LOCAL_IMAGE_NAME} ..\n",
    "\n",
    "docker compose up -d\n",
    "\n",
    "sleep 1\n",
    "\n",
    "aws --endpoint-url=http://localhost:4566       \\\n",
    "      kinesis create-stream                    \\\n",
    "      --stream-name ${PREDICTIONS_STREAM_NAME} \\\n",
    "      --shard-count 1\n",
    "\n",
    "pipenv run python test_docker.py\n",
    "\n",
    "ERROR_CODE=$?\n",
    "\n",
    "if [ ${ERROR_CODE} != 0 ]; then\n",
    "    docker compose logs\n",
    "    docker compose down\n",
    "fi\n",
    "\n",
    "pipenv run python test_kinesis.py\n",
    "\n",
    "ERROR_CODE=$?\n",
    "\n",
    "if [ ${ERROR_CODE} != 0 ]; then\n",
    "    docker compose logs\n",
    "    docker compose down\n",
    "fi\n",
    "\n",
    "docker compose down\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa1e1c0",
   "metadata": {},
   "source": [
    "and then we can test everything by running\n",
    "\n",
    "```\n",
    "$ ./run.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d9b463",
   "metadata": {},
   "source": [
    "## 6.4 - Code quality: linting and formatting\n",
    "\n",
    "Code quality refers to the overall quality, readability and maintainability of your code. **Linting** and **formatting** are two essential practices in Python that contribute to improving code quality.\n",
    "\n",
    "- **Linting** is the process of (statically) analyzing your code against a set of predefined rules or coding standards for potential stylistic inconsistencies and suspicious coding patterns. The most popular linter is ```pylint```, but there are other alternatives like ```flake8``` (faster) or ```ruff``` (written in Rust, much much faster).\n",
    "\n",
    "- **Formatting** ensures that, when different developers may have different preferences for indentation, line length, spacing, ... etc, the code is organized and presented in a uniform manner, making it easier to read and understand. The most widely used Python code formatter is ```black```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead8fb5f",
   "metadata": {},
   "source": [
    "### 6.4.1 Linting\n",
    "\n",
    "In Python, it is recommended to follow the [PEP8 guidelines](https://pep8.org/). This ensures clean, standard code formats and helps code readability. We may want styled code, however, conforming to the PEP8 style manually may be cumbersome. So we can use ```pylint``` instead. \n",
    "\n",
    "To install ```pylint``` within the virtual environment in ```notes-code/```:\n",
    "\n",
    "```\n",
    "$ pipenv install --dev pylint\n",
    "```\n",
    "We can now test it on the ```model.py``` script:\n",
    "\n",
    "```\n",
    "$ pipenv shell\n",
    "$ pylint model.py\n",
    "```\n",
    "which will output:\n",
    "```\n",
    "************* Module model\n",
    "model.py:1:0: C0114: Missing module docstring (missing-module-docstring)\n",
    "...\n",
    "model.py:106:0: C0116: Missing function or method docstring (missing-function-docstring)\n",
    "\n",
    "-----------------------------------\n",
    "Your code has been rated at 8.12/10\n",
    "\n",
    "```\n",
    "You can test all the files in the folder with \n",
    "\n",
    "```\n",
    "$ pylint --recursive=y .\n",
    "```\n",
    "There is also a plugin for ```pylint``` in PyCharm, so that you can analyse the files directly in the IDE (make sure first to install ```pylint``` in the environment that you are currently using):\n",
    "\n",
    "![title](images/pylint.png)\n",
    "\n",
    "You can add the file ```notes-code/.pylintrc``` to configure the linter:\n",
    "\n",
    " - ```~/.pylintrc``` for default user configuration\n",
    " - ```<your project>/.pylintrc``` for default project configuration (used when you'll run ```pylint <your project>```)\n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "[MESSAGES CONTROL]\n",
    "\n",
    "disable=missing-function-docstring,\n",
    "        missing-final-newline,\n",
    "        missing-class-docstring\n",
    "```\n",
    "where the ```.pylintrc``` above allows you to disable specific checks:\n",
    "- when a function or method does not have a docstring\n",
    "- when there is no final newline at the end of a file\n",
    "- classes that lack a docstring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b552f761",
   "metadata": {},
   "source": [
    "There is another alternative: to use a ```pyproject.toml``` file. The ```pyproject.toml``` file can contain configurations for various tools used in the project. For example, you can specify code linters, formatters, and other development tools along with their respective configurations. This avoids using a configuration file for each of them (e.g. ```.pylintrc``` for ```pylint```, etc). So far we only have ```pylint```, so it would look like this:\n",
    "\n",
    "```toml\n",
    "[tool.pylint.messages_control]\n",
    "\n",
    "disable = [\n",
    "    \"missing-function-docstring\",\n",
    "    \"missing-final-newline\",\n",
    "    \"missing-class-docstring\",\n",
    "]\n",
    "```\n",
    "Don't forget to remove the previous ```.pylintrc``` file if you are using ```pyproject.toml```.\n",
    "\n",
    "To ignore certain errors in certain areas of the code, we use ```# pylint: disable=[ERROR CODE]``` blocks. E.g., errors related to a class:\n",
    "\n",
    "```python\n",
    "# ...\n",
    "\n",
    "class KinesisCallback:\n",
    "    # pylint: disable=too-few-public-methods\n",
    "    \n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3028a6",
   "metadata": {},
   "source": [
    "### 6.4.2 Formatting\n",
    "\n",
    "For formatting Python code, the most common tool is ```black```. From the docs:\n",
    "\n",
    "*Black is the uncompromising Python code formatter. By using it, you agree to cede control over minutiae of hand-formatting. In return, Black gives you speed, determinism, and freedom from pycodestyle nagging about formatting. You will save time and mental energy for more important matters.*\n",
    "\n",
    "*Blackened code looks the same regardless of the project you're reading. Formatting becomes transparent after a while and you can focus on the content instead.*\n",
    "\n",
    "To install it:\n",
    "\n",
    "```\n",
    "$ pipenv install --dev black isort\n",
    "```\n",
    "\n",
    "Additionally, the ```isort``` library allows you to automate the process of organizing import statements and reduce the time spent manually sorting imports. It is often used in conjunction with ```black```.\n",
    "\n",
    "Now you can type (within virtual env):\n",
    "\n",
    "```\n",
    "$ black --diff model.py\n",
    "```\n",
    "This option will show you the changes that black would make to the code. Similarly with ```isort```:\n",
    "```\n",
    "$ isort --diff model.py\n",
    "```\n",
    "Then you can \n",
    "1. run ```$ isort .``` to sort the imports\n",
    "2. run ```$ black .``` to format the code\n",
    "3. run ```pylint --recursive=y .``` to check the code\n",
    "4. run ```pytest tests/``` to (unit) test the code.\n",
    "\n",
    "One trick to avoid Black to put several lines in one line is to add a (redundant) comma at the end of the last entry. For example:\n",
    "\n",
    "```python\n",
    "# ...\n",
    "\n",
    "ride = {\n",
    "        \"PULocationID\": 130,\n",
    "        \"DOLocationID\": 205,\n",
    "        \"trip_distance\": 3.66,\n",
    "    }\n",
    "\n",
    "# ...\n",
    "\n",
    "```\n",
    "\n",
    "We can also add the configuration for ```black``` and ```isort``` to ```pyproject.toml```:\n",
    "\n",
    "```toml\n",
    "[tool.pylint.messages_control]\n",
    "disable = [\n",
    "    \"missing-function-docstring\",   \n",
    "    \"missing-final-newline\",          \n",
    "    \"missing-class-docstring\",\n",
    "]\n",
    "\n",
    "[tool.black]\n",
    "line-length = 88                  # maximum line length to 88\n",
    "target-version = ['py310']        # Python version as Python 3.10\n",
    "skip-string-normalization = true  # Skip string change (''->\"\")\n",
    "\n",
    "[tool.isort]\n",
    "# profile=black  # Use the \"black\" profile for import sorting\n",
    "multi_line_output = 3   # Use a grid-style output for imports\n",
    "length_sort = true  # Sort imports by name length \n",
    "                    # (overrides \"black\" profile)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Finally, to enable ```black``` in Pycharm:\n",
    "\n",
    "1. ```(mlops-zoomcamp) $ pip install 'black[d]'```\n",
    "2. Install BlackConnect plugin\n",
    "3. Follow [instructions 3-5](https://black.readthedocs.io/en/stable/integrations/editors.html)\n",
    "4. Now you can format the currently opened file by selecting ```Code -> Reformat Code```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95971363",
   "metadata": {},
   "source": [
    "## 6.5 - Git pre-commit hooks\n",
    "\n",
    "Running tests, formatting, and linting should occur whenever code changes, but it's often easy to overlook. Therefore, running them automatically before we commit to Git can be beneficial. To do that, we can use **pre-commit hooks** \n",
    "\n",
    "We can install ```pre-commit``` within our virtual environment:\n",
    "\n",
    "```\n",
    "$ pipenv install --dev pre-commit\n",
    "```\n",
    "\n",
    "Pre-commit hooks are specific to each Git repository and are stored in the repository's ```.git/hooks``` directory. However, in our case we have a very big repo (```mlops-zoomcamp```) and we are only interested to set up the pre-commit hooks for our current directory ```notes-code/```. Therefore, we could consider the current directory as a standalone repo by using:\n",
    "\n",
    "```\n",
    "$ git init\n",
    "```\n",
    "\n",
    "which will create a ```.git/``` directory in ```notes-code/``` (this will be removed at the end of this section, now it's used just for explanatory purposes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2035ce",
   "metadata": {},
   "source": [
    "### 6.5.1 Initializing pre-commit-hooks config file\n",
    "\n",
    "Pre-commit (the program we have just installed) uses ```.pre-commit-config.yaml``` to specify what programs will run at every commit (the hooks). We can initialize this file with sample config file:\n",
    "\n",
    "```\n",
    "$ pipenv shell\n",
    "$ pre-commit sample-config > .pre-commit-config.yaml\n",
    "```\n",
    "This file is created in ```notes-code/``` and looks like this:\n",
    "\n",
    "```yaml\n",
    "# See https://pre-commit.com for more information\n",
    "# See https://pre-commit.com/hooks.html for more hooks\n",
    "repos:\n",
    "-   repo: https://github.com/pre-commit/pre-commit-hooks\n",
    "    rev: v3.2.0\n",
    "    hooks:\n",
    "    -   id: trailing-whitespace # check for trailing whitespace\n",
    "    -   id: end-of-file-fixer # ensure proper line endings\n",
    "    -   id: check-yaml # validate YAML files\n",
    "    -   id: check-added-large-files # prevent large files from \n",
    "                                    # being added to the repo\n",
    "```\n",
    "\n",
    "To install the hooks defined above:\n",
    "``` \n",
    "$ pre-commit install\n",
    "```\n",
    "Now you can see that they are installed at ```.git\\hooks\\pre-commit```. It's important to note that pre-commit hooks are only executed on the local repository where they are set up. So when getting a new copy of the repo (clone), you need to run ```$ pre-commit install```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46c540c",
   "metadata": {},
   "source": [
    "### 6.5.2 Launch pre-commit\n",
    "\n",
    "To apply the hooks defined in ```.pre-commit-config.yaml``` to the all untracked files:\n",
    "\n",
    "```\n",
    "$ git add .\n",
    "$ git commit -m \"initial commit\"\n",
    "```\n",
    "\n",
    "which will apply the hooks and then commit. *Important:* the files modified by the application of the hooks need to be commited again. Thus:\n",
    "\n",
    "```\n",
    "$ git add .\n",
    "$ git commit -m \"fixes from pre-commit default\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8080293a",
   "metadata": {},
   "source": [
    "### 6.5.3 Adding more hooks to config file\n",
    "\n",
    "Now we want to add ```black```, ```isort```, ```pylint``` and ```pytest``` so that they are applied before each commit. Therefore, the ```.pre-commit-config.yaml``` results:\n",
    "\n",
    "```yaml\n",
    "# See https://pre-commit.com for more information\n",
    "# See https://pre-commit.com/hooks.html for more hooks\n",
    "repos:\n",
    "- repo: https://github.com/pre-commit/pre-commit-hooks\n",
    "  rev: v3.2.0\n",
    "  hooks:\n",
    "    - id: trailing-whitespace\n",
    "    - id: end-of-file-fixer\n",
    "    - id: check-yaml\n",
    "    - id: check-added-large-files\n",
    "- repo: https://github.com/pycqa/isort\n",
    "  rev: 5.10.1\n",
    "  hooks:\n",
    "    - id: isort\n",
    "      name: isort (python)\n",
    "- repo: https://github.com/psf/black\n",
    "  rev: 22.6.0\n",
    "  hooks:\n",
    "    - id: black\n",
    "      language_version: python3.10\n",
    "- repo: local\n",
    "  hooks:\n",
    "    - id: pylint\n",
    "      name: pylint\n",
    "      entry: pylint\n",
    "      language: system\n",
    "      types: [python]\n",
    "      args: [\n",
    "        \"-rn\", # Only display messages\n",
    "        \"-sn\", # Don't display the score\n",
    "        \"--recursive=y\"\n",
    "      ]\n",
    "- repo: local\n",
    "  hooks:\n",
    "    - id: pytest-check\n",
    "      name: pytest-check\n",
    "      entry: pytest\n",
    "      language: system\n",
    "      pass_filenames: false\n",
    "      always_run: true # always run regardless of file changes \n",
    "      args: [\n",
    "        \"tests/\"\n",
    "      ]\n",
    "\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "\n",
    "- The ```repos``` section defines a list of repositories from which the hooks will be downloaded.\n",
    "- Remote repos:\n",
    "  - ```https://github.com/pre-commit/pre-commit-hooks``` It contains a collection of useful pre-commit hooks for various purposes. \n",
    "  - ```https://github.com/pycqa/isort``` It contains ```isort```\n",
    "  - ```https://github.com/psf/black``` It contains ```black```\n",
    "- Local repos (hooks are located locally on your system)\n",
    "  - ```pylint``` for running the Pylint code analysis tool\n",
    "  - ```pytest-check``` for running the Pytest framework\n",
    "  \n",
    "Do not forget to remove the standalone repo ```notes-code/.git```:\n",
    "\n",
    "```\n",
    "$ rm -rf .git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c6ed43",
   "metadata": {},
   "source": [
    "## 6.6 - Makefiles and make\n",
    "\n",
    "### 6.6.1 Brief introduction\n",
    "\n",
    "By utilizing ```make``` and ```Makefile``` in your Python project, you can automate repetitive tasks.\n",
    "\n",
    "- ```Makefile``` contains a set of rules, called *targets* (e.g. compiling the code, running tests, generating documentation), along with their dependencies and associated commands. \n",
    "- By running the ```make``` command, you can automatically build or perform specific tasks in your project based on these rules\n",
    "\n",
    "Normally in Linux systems ```make``` comes pre-installed. For example, we can create ```notes-code/Makefile```:\n",
    "\n",
    "```python\n",
    "run:           # target\n",
    "    echo 123   # Requires Tabs\n",
    "```\n",
    "where ```run``` is an alias (target). Now when ```$ make run``` is executed, ```echo 123``` is executed as a result. We can also make aliases depend on other aliases:\n",
    "\n",
    "```python\n",
    "test-1:                # target \n",
    "    echo '1st test'\n",
    "test-2:                # target\n",
    "    echo '2nd test'\n",
    "run: test-1 test-2     # target (dependency on test-1, test-2)\n",
    "    echo 'running tests'\n",
    "```\n",
    "where ```run``` depends on ```test-1``` and ```test-2```, and when ```$ make run``` is executed, all ```echo '1st test'```, ```echo '2nd test'```, ```echo 'running tests'``` are executed in that order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0525112c",
   "metadata": {},
   "source": [
    "### 6.6.2 ```Makefile``` in a Python project\n",
    "\n",
    "In our case, we want to run *tests* (unit tests and integration tests) and *quality checks* (pylint, black, isort) before running the program or commiting or deploying to AWS. To do so we can make use of the following ```Makefile```: \n",
    "\n",
    "```python\n",
    "# current date and time (used to tag the Docker image)\n",
    "LOCAL_TAG:=$(shell date +\"%Y-%m-%d-%H-%M\")\n",
    "# name of the Docker image\n",
    "LOCAL_IMAGE_NAME:=stream-model-duration:${LOCAL_TAG}\n",
    "\n",
    "# This target is responsible for running the (unit) tests \n",
    "test:\n",
    "\tpytest tests/\n",
    "\n",
    "# This target performs several code quality checks\n",
    "quality_checks:\n",
    "\tisort .\n",
    "\tblack .\n",
    "\tpylint --recursive=y .\n",
    "\n",
    "# This target first ensures that the quality checks and tests\n",
    "# have passed, and then it builds the Docker image.\n",
    "build: quality_checks test\n",
    "\tdocker build -t ${LOCAL_IMAGE_NAME} .\n",
    "\n",
    "# This target is responsible for running integration tests \n",
    "# using the built Docker image\n",
    "integration_test: build\n",
    "\tLOCAL_IMAGE_NAME=${LOCAL_IMAGE_NAME} bash integraton-test/run.sh\n",
    "\n",
    "# This target is responsible for publishing the Docker image, \n",
    "# typically to a container registry.\n",
    "publish: build integration_test\n",
    "\tLOCAL_IMAGE_NAME=${LOCAL_IMAGE_NAME} bash scripts/publish.sh\n",
    "\n",
    "# This target is typically used to set up the project \n",
    "# environment initially.\n",
    "setup:\n",
    "\tpipenv install --dev\n",
    "\tpre-commit install\n",
    "```\n",
    "\n",
    "By defining these targets and their associated commands in the ```Makefile```, you can easily run specific tasks by executing the ```make``` command followed by the target name. For example:\n",
    "- ```$ make test``` will execute the tests\n",
    "- ```$ make publish``` will build the image, run integration tests, and publish the Docker image.\n",
    "\n",
    "Note: ```notes-code/scripts/publish.sh``` is defined as:\n",
    "\n",
    "```bash\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "echo \"publishing image ${LOCAL_IMAGE_NAME} to Amazon ECR repository...\"\n",
    "```\n",
    "\n",
    "It's actually not publishing to ECR but just printing a message."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlops-zoomcamp]",
   "language": "python",
   "name": "conda-env-mlops-zoomcamp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
